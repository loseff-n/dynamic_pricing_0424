{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from io import StringIO\n",
    "import mysql.connector as mysql\n",
    "\n",
    "from statistics import mode\n",
    "from transliterate import translit\n",
    "from workalendar.europe import Russia\n",
    "\n",
    "from clickhouse_driver import Client\n",
    "from sqlalchemy import create_engine, text\n",
    "from clickhouse_sqlalchemy import make_session\n",
    "from datetime import datetime, date, time\n",
    "import ast  # Модуль ast (Abstract Syntax Trees) для преобразования строк в списки\n",
    "from math import radians, sin, cos, sqrt, atan2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np.__version__ 1.26.4\n",
      "pd.__version__ 2.0.3\n",
      "re.__version__ 2.2.1\n",
      "requests.__version__ 2.31.0\n",
      "mysql.__version__ 8.3.0\n",
      "transliterate.__version__ 1.10.2\n",
      "workalendar.__version__ 17.0.0\n",
      "clickhouse_driver.__version__ 0.2.7\n",
      "sqlalchemy.__version__ 2.0.28\n",
      "clickhouse_sqlalchemy.__version__ 0.3.0\n"
     ]
    }
   ],
   "source": [
    "print('np.__version__', np.__version__)\n",
    "print('pd.__version__', pd.__version__)\n",
    "print('re.__version__', re.__version__)\n",
    "print('requests.__version__', requests.__version__)\n",
    "print('mysql.__version__', mysql.__version__)\n",
    "from transliterate import __version__\n",
    "print('transliterate.__version__', __version__)\n",
    "from workalendar import __version__\n",
    "print('workalendar.__version__', __version__)\n",
    "from clickhouse_driver import __version__\n",
    "print('clickhouse_driver.__version__', __version__)\n",
    "from sqlalchemy import __version__\n",
    "print('sqlalchemy.__version__', __version__)\n",
    "from clickhouse_sqlalchemy import __version__\n",
    "print('clickhouse_sqlalchemy.__version__', __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_from_synDB(query):\n",
    "    conn = mysql.connect(\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "            )\n",
    "    # Чтение данных из базы данных и запись их в DataFrame\n",
    "    data = pd.read_sql_query(query, conn)\n",
    "\n",
    "    # Закрытие соединения с базой данных\n",
    "    conn.close()\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_order_lines():\n",
    "\n",
    "    order_lines = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                            ;\"\"\")\n",
    "    \n",
    "    print('order_lines.shape', order_lines.shape)\n",
    "    return order_lines\n",
    "\n",
    "def get_orders():\n",
    "\n",
    "    orders = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                    ;\"\"\")\n",
    "    \n",
    "    print('orders.shape', orders.shape)\n",
    "    return orders\n",
    "\n",
    "\n",
    "def get_products():\n",
    "\n",
    "    products = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                          ;\"\"\")\n",
    "    \n",
    "    print('products.shape', products.shape)\n",
    "    return products\n",
    "\n",
    "\n",
    "def get_clients():\n",
    "\n",
    "    clients = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                        ;\"\"\")\n",
    "\n",
    "    print('clients.shape', clients.shape)\n",
    "    return clients\n",
    "\n",
    "\n",
    "# Функция для вычисления разницы цен с конкурентами (использовать, если не будет решена проблема с нулевыми ценами)\n",
    "def calculate_price_diff(row, price_competitor_column, column_syn_ru):\n",
    "    if np.isnan(row[price_competitor_column]) or np.isnan(row[column_syn_ru]) or row[column_syn_ru] == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        result = (row[price_competitor_column] - row[column_syn_ru]) / row[column_syn_ru]\n",
    "        if result > 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return round(result, 2)\n",
    "\n",
    "def get_pricing():\n",
    "\n",
    "    pricing = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                        ;\"\"\")\n",
    "\n",
    "    pricing.drop_duplicates(inplace=True)\n",
    "\n",
    "    pricing['date'] = pd.to_datetime(pricing['date'])\n",
    "    # Сортируем по дате\n",
    "    pricing.sort_values(by='date', inplace=True)\n",
    "    # Приводим к единому формату id\n",
    "    pricing['id'] = pricing['id'].astype(float).astype(int)\n",
    "\n",
    "    # Преобразуем столбец 'price_ozon' и 'price_wb' в тип данных float, с заменой некорректных значений на NaN\n",
    "    pricing['price_ozon'] = pd.to_numeric(pricing['price_ozon'], errors='coerce')\n",
    "    pricing['price_wb'] = pd.to_numeric(pricing['price_wb'], errors='coerce')\n",
    "\n",
    "    # Считаем относительную разницу цен (отрицательное значение значит цену ниже, чем у , положительное - выше)\n",
    "    pricing['price_ozon_diff'] = pricing.apply(lambda row: calculate_price_diff(row, 'price_ozon', 'price_syn_ru'), axis=1)\n",
    "    pricing['price_wb_diff'] = pricing.apply(lambda row: calculate_price_diff(row, 'price_wb', 'price_syn_ru'), axis=1)\n",
    "\n",
    "    # Создаем новый столбец, содержащий наибольшую разницу c 'price_ozon_diff' и 'price_wb_diff',\n",
    "    # предполагая, что чем ниже цена на какой-то из этих площадок, тем более нечевствителен к скидкам покупающий на * клиент\n",
    "    pricing['price_diff'] = abs(pricing[['price_ozon_diff', 'price_wb_diff']].min(axis=1))\n",
    "    \n",
    "    print('pricing.shape', pricing.shape)\n",
    "    return pricing\n",
    "\n",
    "def get_products_visits():\n",
    "\n",
    "    products_visits = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                             ;\"\"\")\n",
    "\n",
    "    products_visits['date_time'] = pd.to_datetime(products_visits['date_time'])\n",
    "    products_visits['ProductIdsWebsite'] = products_visits['ProductIdsWebsite'].astype('int32')\n",
    "\n",
    "    return products_visits\n",
    "\n",
    "def get_visits():\n",
    "\n",
    "    engine = create_engine(\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                           \", connect_args={'connect_timeout': 1800000})\n",
    "\n",
    "    # Выгружаем только значимые столбцы с ограничение Bounce == 0 (если 1 - то отказ, их не учитываем)\n",
    "    # Исключаем данные до 20-08-2022\n",
    "    query = text(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "    \"\"\")\n",
    "    session = make_session(engine)\n",
    "    result = session.execute(query).fetchall()\n",
    "    data = [row for row in result]\n",
    "    visits = pd.DataFrame(data)\n",
    "\n",
    "    print('visits.shape', visits.shape)\n",
    "    return visits\n",
    "\n",
    "def get_yandex_client_ids():\n",
    "    yandex_client_ids = get_from_synDB(\"\"\"\n",
    "'''\n",
    "ИСКЛЮЧЕНЫ ДАННЫЕ ПО NDA\n",
    "'''\n",
    "                                ;\"\"\")\n",
    "\n",
    "    print('yandex_client_ids.shape', yandex_client_ids.shape)\n",
    "\n",
    "    # Убираем дубликаты, возможные из-за отсутсвия столбца id\n",
    "    yandex_client_ids.drop_duplicates(inplace=True)\n",
    "    \n",
    "    print('yandex_client_ids.shape', yandex_client_ids.shape)\n",
    "\n",
    "    return yandex_client_ids\n",
    "\n",
    "def get_nearest_pricing_time(df, pricing, date_time_column_name):\n",
    "    price_list = pricing['date'].unique()\n",
    "    # Создаем колонку с ближайшей датой назначения цены\n",
    "    for i in df.index:\n",
    "        try:\n",
    "            df.loc[i, 'nearest_pricing_date'] = pd.to_datetime(price_list[price_list < df.loc[i, date_time_column_name]][-1])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print('get_nearest_pricing_time.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "def preprocess_products_for_merge(products):\n",
    "    # Меняем тип данных для корректного мерджа\n",
    "    products['ProductIdsWebsite'] = products['ProductIdsWebsite'].astype('int').astype('str')\n",
    "\n",
    "    # Создаем столбец с кол-вом присвоенных категория для каждого товара\n",
    "    def count_elements_in_list(lst):\n",
    "        if lst is None:  # Добавляем проверку на значение None\n",
    "            return 0\n",
    "        else:\n",
    "            return len(lst)\n",
    "    products['number_of_categories'] =  products['ProductCategoriesIdsWebsite'].apply(count_elements_in_list)\n",
    "\n",
    "    # Заменяем на основную категорию\n",
    "    products['ProductCategoriesIdsWebsite'] = products['ProductCategoriesIdsWebsite'].str.split('|', expand=True)[0]\n",
    "\n",
    "    print('products.shape', products.shape)\n",
    "    return products    \n",
    "\n",
    "def preprocess_yandex_client_ids(yandex_client_ids):\n",
    "    yandex_client_ids.drop_duplicates(inplace=True)\n",
    "    # Приводим в правильный формат даты\n",
    "    yandex_client_ids['authorized_at'] = pd.to_datetime(yandex_client_ids['authorized_at'])\n",
    "    # Корректно отображаем пустые значения\n",
    "    yandex_client_ids.loc[:,['yandex_id', 'mindbox_id']] = yandex_client_ids.loc[:,['yandex_id', 'mindbox_id']].replace('', np.nan)\n",
    "    # Вместо 'authorized_at' введем:\n",
    "    # Первая авторизация\n",
    "    yandex_client_ids['first_authorized_at'] = yandex_client_ids.groupby('mindbox_id')['authorized_at'].transform('min')\n",
    "    # Последняя авторизация\n",
    "    yandex_client_ids['last_authorized_at'] = yandex_client_ids.groupby('mindbox_id')['authorized_at'].transform('max')\n",
    "    # Число проведенных на сайте дней\n",
    "    yandex_client_ids['authorized_days'] = yandex_client_ids['authorized_at'].dt.date\n",
    "    yandex_client_ids['authorized_days'] = yandex_client_ids.groupby('mindbox_id')['authorized_days'].transform('nunique')\n",
    "    # Число авторизаций website_id\n",
    "    yandex_client_ids['authorized_website_id_count'] = yandex_client_ids.groupby('mindbox_id')['website_id'].transform('count')\n",
    "    yandex_client_ids.drop(columns=['website_id', 'authorized_at'], inplace=True)\n",
    "    yandex_client_ids.drop_duplicates(inplace=True)\n",
    "    # Сколько прошло дней с момента последней авторизации\n",
    "    yandex_client_ids['days_since_last_authorized'] = (datetime.now() - yandex_client_ids['last_authorized_at']).dt.days\n",
    "\n",
    "    print('yandex_client_ids.shape', yandex_client_ids.shape)\n",
    "\n",
    "    return yandex_client_ids\n",
    "\n",
    "def prepare_yandex_client_ids_to_merge(yandex_client_ids):\n",
    "\n",
    "    # Готовим данные к мерджу\n",
    "    yandex_client_ids_to_merge = yandex_client_ids.drop(columns=['yandex_id', 'first_authorized_at', 'last_authorized_at'])\n",
    "    yandex_client_ids_to_merge.drop_duplicates(inplace=True)\n",
    "    yandex_client_ids_to_merge['mindbox_id'] = pd.to_numeric(yandex_client_ids_to_merge['mindbox_id'])\n",
    "    yandex_client_ids_to_merge.rename(columns={'mindbox_id': 'CustomerIdsMindboxId'}, inplace=True)\n",
    "\n",
    "    print('yandex_client_ids_to_merge.shape', yandex_client_ids_to_merge.shape)\n",
    "\n",
    "    return yandex_client_ids_to_merge\n",
    "\n",
    "# Создаем словарь замены неуникальных id юзеров на один уникальный\n",
    "def make_id_dic(id_series):\n",
    "    id_dic = {}\n",
    "    for row in id_series:\n",
    "        if row[0] in id_dic.keys():\n",
    "            for i in range(1, len(row)):\n",
    "                id_dic[row[i]] = id_dic[row[0]]\n",
    "        else:\n",
    "            for i in range(1, len(row)):\n",
    "                id_dic[row[i]] = row[0]\n",
    "    # Делаем еще один проход, чтобы убрать лишние промежуточные ключи, созданные при первом проходе\n",
    "    for key, value in id_dic.items():\n",
    "        if value in id_dic.keys():\n",
    "            id_dic[key] = id_dic[value]\n",
    "    return id_dic\n",
    "\n",
    "def make_yandex_client_ids_dic(yandex_client_ids):\n",
    "    # Удаляем оставшиеся NAN\n",
    "    yandex_client_ids = yandex_client_ids.dropna()\n",
    "\n",
    "    # Дубликатов меньше в yandex_id, поэтому создадим справочник повторных присвоений mindbox_id и дальше уникальным клиентам присвоим уникальны mindbox_id \n",
    "    # (поскольку иногда возникает несколько для одного клиента)\n",
    "    # В дальнейшем используя данный словарь, нужно будет избавиться от дубликатов и в других данных\n",
    "    mindbox_id_series = yandex_client_ids[yandex_client_ids['yandex_id'].duplicated(keep=False)].groupby('yandex_id')['mindbox_id'].apply(list)\n",
    "\n",
    "    # Сортируем одинаково, чтобы не возникало незаметных дубликатов\n",
    "    mindbox_id_series = mindbox_id_series.apply(lambda x: sorted(x))\n",
    "\n",
    "    # Удаляем дубликаты\n",
    "    mindbox_id_series.drop_duplicates(keep='last', inplace=True)\n",
    "\n",
    "    # Создаем словарь для замены неуникальных mindbox_id\n",
    "    mindbox_id_dic = make_id_dic(mindbox_id_series)\n",
    "\n",
    "    # Унифицируем mindbox_id\n",
    "    yandex_client_ids.loc[:,'mindbox_id'] = yandex_client_ids['mindbox_id'].apply(lambda x: mindbox_id_dic[x] if x in mindbox_id_dic.keys() else x)\n",
    "    yandex_client_ids.drop_duplicates(subset=['yandex_id', 'mindbox_id'], keep='last', inplace=True)\n",
    "    yandex_client_ids.reset_index(inplace=True)\n",
    "\n",
    "    # Создаем словарь для замены yandex_id на mindbox_id\n",
    "    yandex_id_dic = {}\n",
    "    for i in range(len(yandex_client_ids)):\n",
    "        yandex_id_dic[yandex_client_ids.loc[i, 'yandex_id']] = yandex_client_ids.loc[i, 'mindbox_id']\n",
    "\n",
    "    print('len(yandex_id_dic)', len(yandex_id_dic))\n",
    "\n",
    "    return yandex_id_dic\n",
    "\n",
    "    # Функция для удаления пустых значений из списка\n",
    "def remove_empty_values(coupon_list):\n",
    "    return len(list(filter(lambda x: x != '', coupon_list)))\n",
    "\n",
    "# Функция для извлечения доменных имен с использованием регулярных выражений\n",
    "def extract_domain(url):\n",
    "    if pd.notna(url):\n",
    "        match = re.search(r'[^.]+[.][^.]+$', url)\n",
    "        return match.group() if match else None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "def preprocess_visits(visits, yandex_id_dic):\n",
    "    # Создаем столбец с id_mindbox\n",
    "    visits.loc[:,'ClientID'] = visits['ClientID'].astype('str')\n",
    "    visits.loc[:,'CustomerIdsMindboxId'] = visits['ClientID'].apply(lambda x: yandex_id_dic[x] if x in yandex_id_dic.keys() else np.nan)\n",
    "    # Исключаем из данных пользователей, которых нет в базе mindbox')\n",
    "    visits = visits[visits['CustomerIdsMindboxId'].notna()]\n",
    "    # Приводим данные к нужному виду')\n",
    "    visits.loc[:,'Date'] = pd.to_datetime(visits['Date'])\n",
    "    # Восстановим NAN там, где они фактически присутствуют')\n",
    "    visits.loc[:,'LastReferalSource'] = visits['LastReferalSource'].replace('', np.nan)\n",
    "    visits.loc[:,'LastSearchEngineRoot'] = visits['LastSearchEngineRoot'].replace('', np.nan)\n",
    "    visits.loc[:,'LastSocialNetwork'] = visits['LastSocialNetwork'].replace('', np.nan)\n",
    "    visits.loc[:,'RegionCity'] = visits['RegionCity'].replace('', np.nan)\n",
    "    # Создаем паттерн регулярного выражения для поиска текста между '(' и ','\n",
    "    pattern = r'\\((.*?)\\,'\n",
    "    # Функция, которая извлекает первое значение между '(' и ','\n",
    "    def extract_value(text):\n",
    "        match = re.search(pattern, text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    # Применяем функцию к столбцу visits['LastSourceEngine']\n",
    "    visits.loc[:,'LastSourceEngine'] = visits['LastSourceEngine'].apply(lambda x: extract_value(x))\n",
    "    # Считаем кол-во примененных купонов\n",
    "    # От str переходим к list')\n",
    "    visits.loc[:,'PurchaseCoupon'] = visits['PurchaseCoupon'].apply(ast.literal_eval)\n",
    "    # Кол-во успешных применений купона')\n",
    "    visits.loc[:,'PurchaseCoupon'] = visits['PurchaseCoupon'].apply(remove_empty_values)\n",
    "    # Из PurchaseID извлекаем число заказов (в самих ID здесь смысла нет)')\n",
    "    visits.loc[:,'PurchaseID'] = visits['PurchaseID'].apply(ast.literal_eval)\n",
    "    visits.loc[:,'PurchaseID'] = visits['PurchaseID'].apply(lambda x: len(x))\n",
    "    # Считаем кол-во WatchIDs (поскольку сами id ничему не релевантны)')\n",
    "    visits.loc[:,'WatchIDs'] = visits['WatchIDs'].apply(lambda x: len(x))\n",
    "    # Сокращаем значение до домена')\n",
    "    visits.loc[:,'LastReferalSource'] = visits['LastReferalSource'].apply(extract_domain)\n",
    "    # Рассчитываем частоту значений в столбце LastReferalSource')\n",
    "    value_counts = visits['LastReferalSource'].value_counts(normalize=True)\n",
    "    # Заменяем значения, которые встречаются реже, чем в 0.001 случаях, на Other')\n",
    "    threshold = 0.001\n",
    "    visits.loc[:,'LastReferalSource'] = visits['LastReferalSource'].apply(lambda x: x if value_counts.get(x, 0) >= threshold else 'Other')\n",
    "    # Рассчитываем частоту значений в столбце LastSearchEngineRoot')\n",
    "    value_counts = visits['LastSearchEngineRoot'].value_counts(normalize=True)\n",
    "    visits.loc[:,'LastSearchEngineRoot'] = visits['LastSearchEngineRoot'].apply(lambda x: x if value_counts.get(x, 0) >= threshold else 'Other')\n",
    "    # Рассчитываем частоту значений в столбце LastSocialNetwork')\n",
    "    value_counts = visits['LastSocialNetwork'].value_counts(normalize=True)\n",
    "    visits.loc[:,'LastSocialNetwork'] = visits['LastSocialNetwork'].apply(lambda x: x if value_counts.get(x, 0) >= threshold else 'Other')\n",
    "\n",
    "    print('visits.shape', visits.shape)\n",
    "\n",
    "    # Необходимо избавиться от дубликатов в данных. Поскольку метрика выдает интегральные показатели за сутки, ')\n",
    "    # необходимо произвести слияние информации по визитам, совершенным в течение 1 суток\n",
    "    visits = visits.groupby(['Date', 'CustomerIdsMindboxId']).agg({'DeviceCategory':'last', 'IsNewUser':'max', 'LastReferalSource':'last',\n",
    "       'LastSearchEngineRoot':'last', 'LastSocialNetwork':'last', 'LastTrafficSource':'last',\n",
    "       'PageViews':'sum', 'RegionCity':'last',\n",
    "       'VisitDuration':'sum', 'VisitID':'count', 'WatchIDs':'sum', 'LastSourceEngine':'last', \n",
    "       'PurchaseCoupon':'sum', 'PurchaseID':'sum'}).reset_index()\n",
    "\n",
    "    print('visits.shape', visits.shape)\n",
    "\n",
    "    # Готовим к мерджу')\n",
    "    visits = visits.rename(columns={'Date':'OrderDate',\n",
    "                        'VisitID':'VisitIDNums', 'WatchIDs':'WatchIDsNums', \n",
    "                        'PurchaseCoupon':'PurchaseCouponApplied',\n",
    "                        'PurchaseID':'PurchaseNums'})\n",
    "    visits['CustomerIdsMindboxId'] = pd.to_numeric(visits['CustomerIdsMindboxId'])\n",
    "\n",
    "    return visits\n",
    "\n",
    "def get_sex_names():\n",
    "    file_id = '1npMAUX-BiaXyVrHG_GogSEthSuQhtdeF'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    file_like_object = StringIO(content)\n",
    "    female_names = [line.lower().strip() for line in file_like_object.readlines()]\n",
    "\n",
    "    file_id = '15VOP4TAW_IWZeSmkEvwpKRyS3ZXR5bKe'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    response = requests.get(url)\n",
    "    content = response.text\n",
    "    file_like_object = StringIO(content)\n",
    "    male_names = [line.lower().strip() for line in file_like_object.readlines()]\n",
    "\n",
    "    return female_names, male_names\n",
    "\n",
    "def contains_name(name, female_names, male_names):\n",
    "    # Функция проверки имени на муж/жен\n",
    "    if isinstance(name, float):\n",
    "        return name\n",
    "    words = name.lower().split()\n",
    "    if any(word in female_names for word in words):\n",
    "        return 'female'\n",
    "    elif any(word in male_names for word in words):\n",
    "        return 'male'\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def contains_surname(name):\n",
    "    # Функция проверки фамилии на муж/жен\n",
    "    if isinstance(name, float):\n",
    "        return name\n",
    "    words = name.lower().split()\n",
    "    if any(word.endswith(('ев', 'ов', 'ин', 'ын')) for word in words):\n",
    "        return 'male'\n",
    "    elif any(word.endswith((\"ева\", \"ова\", \"ина\", 'ына', 'ая')) for word in words):\n",
    "        return 'female'\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def contains_test(name):\n",
    "    # Функция поиска тестовых значений\n",
    "    if isinstance(name, float):\n",
    "        return 1    \n",
    "    if 'тест' in name:\n",
    "        return np.nan\n",
    "    elif 'проверка' in name:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def get_sex(df):\n",
    "\n",
    "    # Определяем пол по окончаниям отчеств:\n",
    "    df['CustomerSex2'] = df.CustomerMiddleName.apply(lambda x: 'male' if isinstance(x, str) \n",
    "                                                    and x.lower().endswith(('ич', 'ыч', 'лы'))\n",
    "                                                    else 'female' if isinstance(x, str) \n",
    "                                                    and x.lower().endswith((\"на\", \"зы\", \"ва\"))\n",
    "                                                    else np.nan)\n",
    "    df['CustomerSex'] = df['CustomerSex'].fillna(df['CustomerSex2'])\n",
    "\n",
    "    # Определяем пол по окончаниям фамилий:\n",
    "    df['CustomerSex2'] = df.CustomerLastName.apply(lambda x: 'male' if isinstance(x, str) \n",
    "                                                and x.lower().endswith(('ев', 'ов', 'ин', 'ын'))\n",
    "                                                else 'female' if isinstance(x, str) \n",
    "                                                and x.lower().endswith((\"ева\", \"ова\", \"ина\", 'ына', 'ая'))\n",
    "                                                else np.nan)\n",
    "    df['CustomerSex'] = df['CustomerSex'].fillna(df['CustomerSex2'])\n",
    "\n",
    "    # Проведем транслитерацию имен в кириллицу с использование библиотеки transliterate\n",
    "    df['CustomerFirstName_mod'] = df['CustomerFirstName'].fillna('')\n",
    "    df['CustomerFirstName_mod'] = df['CustomerFirstName_mod'].apply(lambda x: translit(x.lower(), 'ru'))\n",
    "\n",
    "    # Вычленим пол из имен\n",
    "    female_names, male_names = get_sex_names()\n",
    "    df['CustomerSex2'] = df['CustomerFirstName_mod'].apply(lambda x: contains_name(x, female_names, male_names))\n",
    "    df['CustomerSex'] = df['CustomerSex'].fillna(df['CustomerSex2'])\n",
    "\n",
    "    # Проверка окончаний на муж/жен\n",
    "    df['CustomerSex2'] = df['CustomerFirstName_mod'].apply(lambda x: 'male' if isinstance(x, str) and ('ич' in x.lower() or 'ыч' in x.lower())\n",
    "                                else 'female' if isinstance(x, str) and (\"на\" in x.lower() or \"зы\" in x.lower())\n",
    "                                else 'female' if isinstance(x, str) and x.lower().endswith((\"а\", \"я\"))\n",
    "                                else 'male' if isinstance(x, str) and x.lower().endswith(('й', 'р', 'н', 'в'))\n",
    "                                else np.nan)\n",
    "    df['CustomerSex'] = df['CustomerSex'].fillna(df['CustomerSex2'])\n",
    "\n",
    "    # Вычленим пол из фамилий в поле имени\n",
    "    df['CustomerSex2'] = df['CustomerFirstName_mod'].apply(lambda x: contains_surname(x))\n",
    "    df['CustomerSex'] = df['CustomerSex'].fillna(df['CustomerSex2'])\n",
    "\n",
    "    # Меняем на булевое: 1 - мужчина (небольшое число пропусков признаем женщинами, учитывая сильный перевес)\n",
    "    df['IsMale'] = df['CustomerSex'] == 'male'\n",
    "\n",
    "    print('get_sex.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def delete_test(df):\n",
    "    # Удалим тестовые значения\n",
    "    df['CustomerFirstName_mod'] = df['CustomerFirstName_mod'].apply(lambda x: contains_test(x))\n",
    "\n",
    "    # Удалим строки, в которых столбец 'CustomerLastName' содержит слово, подобное \"Тест\"\n",
    "    pattern = re.compile(r'.*Тест.*')\n",
    "    df.loc[:, 'CustomerFirstName_mod'] = df['CustomerLastName'].fillna('')\n",
    "    df = df[~df['CustomerFirstName_mod'].str.contains(pattern)]\n",
    "\n",
    "    # Удалим строки, в которых столбец 'CustomerMiddleName' содержит слово, подобное \"Тест\"\n",
    "    pattern = re.compile(r'.*Тест.*')\n",
    "    df.loc[:, 'CustomerFirstName_mod'] = df['CustomerMiddleName'].fillna('')\n",
    "    df = df[~df['CustomerFirstName_mod'].str.contains(pattern)]\n",
    "\n",
    "    # Удалим строки, в которых столбец 'CustomerFirstName' содержит слово, подобное \"Тест\"\n",
    "    pattern = re.compile(r'.*Тест.*')\n",
    "    df.loc[:, 'CustomerFirstName_mod'] = df['CustomerFirstName'].fillna('')\n",
    "    df = df[~df['CustomerFirstName_mod'].str.contains(pattern)]\n",
    "\n",
    "    print('delete_test.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "def extract_city(input_string):\n",
    "    # Выделим города, очистив от лишних символов, названий регионов, улиц и т.д.\n",
    "    if isinstance(input_string, str) and 'г ' in input_string:\n",
    "        # Ищем 'г '\n",
    "        index = input_string.find('г ')\n",
    "        if index != -1:\n",
    "            # Проверка на наличие пробела перед 'г '\n",
    "            if index == 0 or (index > 0 and (input_string[index - 1] == ' ' or input_string[index - 1] == ',')):\n",
    "                # Выделяем слова после 'г ' без запятой\n",
    "                remaining_text = input_string[index + 2:]\n",
    "                match = re.search(r'([\\w-]+(?: [\\w-]+)?)', remaining_text)\n",
    "                if match:\n",
    "                    return match.group(1).strip()\n",
    "    return input_string\n",
    "\n",
    "def extract_city_2(input_string):\n",
    "    if isinstance(input_string, str) and 'г ' in input_string:\n",
    "        # Найдем 'г '\n",
    "        index = input_string.find('г ')\n",
    "        if index != -1:\n",
    "            # Проверим пробел перед 'г '\n",
    "            if index == 0 or (index > 0 and (input_string[index - 1] == ' ' \n",
    "                                             or input_string[index - 1] == ',')):\n",
    "                # Найдем значения после 'г '\n",
    "                remaining_text = input_string[index + 2:]\n",
    "                match = re.search(r'([\\w-]+(?: [\\w-]+)?)', remaining_text)\n",
    "                if match:\n",
    "                    return match.group(1).strip()\n",
    "    return None\n",
    "\n",
    "def extract_region(input_string):\n",
    "    if (not isinstance(input_string, str)) or len(input_string) < 5:\n",
    "        return None\n",
    "\n",
    "    _obl = input_string.find('обл')\n",
    "    _kray = input_string.find('край')\n",
    "    _resp = input_string.find(' Респ,')\n",
    "    _respublika = input_string.find(' республика')\n",
    "    resp_ = re.search(r'Респ\\s(\\w+)', input_string)\n",
    "    _ao = input_string.find(' АО')\n",
    "    \n",
    "    if 'Москва' in input_string:\n",
    "        return 'Москва'\n",
    "    elif 'Санкт-Петербург' in input_string:\n",
    "        return 'Санкт-Петербург'\n",
    "    elif _obl != -1:\n",
    "        return input_string[:_obl].strip()\n",
    "    elif _kray != -1:\n",
    "        return input_string[:_kray].strip()\n",
    "    elif _resp != -1:\n",
    "        return input_string[:_resp].strip()\n",
    "    elif _respublika != -1:\n",
    "        return input_string[:_respublika].strip()\n",
    "    elif resp_:\n",
    "        return resp_.group(1)\n",
    "    elif _ao != -1:\n",
    "        return input_string[:_ao].strip()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_cities_names():\n",
    "    file_id = '1ZGTMel-tik1z9SfkcNThc-FH5yawhm_4'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    cities_ru_en = pd.read_csv(url, header=None)\n",
    "\n",
    "    return cities_ru_en\n",
    "\n",
    "def get_cities(df):\n",
    "    cities_ru_en = get_cities_names()\n",
    "\n",
    "    df.CustomerCustomFieldsCity = df.CustomerCustomFieldsCity.apply(extract_city)\n",
    "\n",
    "    # RegionCity - город из яндекс метрики\n",
    "    df = df.merge(cities_ru_en, how='left', left_on='RegionCity', right_on=1)\n",
    "    df['CustomerCustomFieldsCity'] = df['CustomerCustomFieldsCity'].fillna(df[0])\n",
    "    df.drop(columns=['RegionCity', 0, 1], inplace=True)\n",
    "\n",
    "    # CustomerCustomFieldsCityOrder\tГород доставки\tobject\t265918\t98.93\t1\t[Санкт-Петербург] - недостоверный \n",
    "    df['CustomerCustomFieldsCity'] = df['CustomerCustomFieldsCity'].fillna(df['CustomerCustomFieldsCityOrder'])\n",
    "\n",
    "    # CustomerCustomFieldsCitySystem\tГород (по номеру телефона)\n",
    "    df = df.merge(cities_ru_en, how='left', left_on='CustomerCustomFieldsCitySystem', right_on=1)\n",
    "    df['CustomerCustomFieldsCity'] = df['CustomerCustomFieldsCity'].fillna(df[0])\n",
    "    df.drop(columns=['CustomerCustomFieldsCityOrder', 'CustomerCustomFieldsCitySystem', 0, 1], inplace=True)\n",
    "\n",
    "    df['City'] = df['OrderCustomFieldsDeliveryAddress'].apply(extract_city_2)\n",
    "\n",
    "    print('get_cities.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "def get_bools(df):\n",
    "    # Заменяем значения на булевые\n",
    "    df['VK_bot'] = df.CustomerCustomFieldsUtmFirst.apply(lambda x: True if x == 'VK_bot' else False) # пришел с вконтакте\n",
    "    df['Site'] = df.CustomerCustomFieldsUtmFirst.apply(lambda x: True if x == 'Site' else False) # пришел с сайта\n",
    "    df['GreenWave'] = df.CustomerCustomFieldsUtmFirst.apply(lambda x: True if x == 'GreenW' else False) # пришел с Зеленой волны\n",
    "    df['consultants'] = df.CustomerCustomFieldsUtmFirst.apply(lambda x: True if x == 'consultants' else False) # привлекли консультанты\n",
    "    df['eco'] = df.CustomerCustomFieldsUtmFirst.apply(lambda x: True if x == 'ecoMobile' or x == 'sborka' else False) # привлечен эко-тематикой\n",
    "    df['consultants_2'] = df.CustomerCustomFieldsUtmMedium.apply(lambda x: True if x == 'consultants' or x == 'consultanys' else False)\n",
    "    df['consultants'] = df['consultants'] + df['consultants_2']\n",
    "    df['consultants'] = df['consultants'].apply(lambda x: True if x >= 1 else False)\n",
    "    df['eco_2'] = df.CustomerCustomFieldsUtmMedium.apply(lambda x: True if x in ['ecoMobile', 'sborka', 'Шоурум', 'Sborka', 'ecodom'] else False)\n",
    "    df['eco'] = df['eco'] + df['eco_2']\n",
    "    df['eco'] = df['eco'].apply(lambda x: True if x >= 1 else False)\n",
    "    df = df.drop(columns=['CustomerCustomFieldsUtmFirst', 'CustomerCustomFieldsUtmMedium', 'consultants_2', 'eco_2'])\n",
    "\n",
    "    # Заменим значения на булевые и переименуем:\n",
    "    # 1, если было взаимодействие offline\n",
    "    # 0 в других случаях\n",
    "    df.CustomerCustomFieldsUtmSource = df.CustomerCustomFieldsUtmSource.apply(lambda x: True if x == 'offline'\n",
    "                                                                            else False)\n",
    "    df.rename(columns={'CustomerCustomFieldsUtmSource': 'offline'}, inplace=True)\n",
    "\n",
    "    print('get_bools.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_regions(df):\n",
    "    # extract region\n",
    "    df['Region'] =  df['OrderCustomFieldsDeliveryAddress'].apply(extract_region)\n",
    "\n",
    "    # Необходимо убрать города и прочий неадекват:\n",
    "    df['Region'] = df['Region'].apply(lambda x: np.nan if str(x).startswith('г') else x)\n",
    "\n",
    "    # Исправляем единичную ошибку с Еврейская А\n",
    "    df['Region'] = df['Region'].apply(lambda x: 'Еврейская' if x == 'Еврейская А' else x)\n",
    "\n",
    "    # Обнулим значения региона, если они прошли с ошибкой (частотность значения меньше 15)\n",
    "    value_counts = df['Region'].value_counts()\n",
    "    values_to_replace = value_counts[value_counts < 15].index\n",
    "    df['Region'] = df['Region'].apply(lambda x: np.nan if x in values_to_replace else x)\n",
    "\n",
    "    # Заполним пустые города (поставив условие, что регион неизвестен, потому что необходимо, \n",
    "    # чтобы не возникло противоречия между городом и регионом \n",
    "    # (такое возможно, поскольку CustomerCustomFieldsCity неточный предсказатель)\n",
    "\n",
    "    df['City'] = df['City'].fillna(df[df['Region'].isna()]['CustomerCustomFieldsCity'])\n",
    "\n",
    "    # Оставшиеся пропуски считаем сельской местностью. Присвоим значение 'Сельские районы'\n",
    "    df['City'] = df['City'].fillna('Сельские районы')\n",
    "\n",
    "    print('process_regions.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_cities_db():\n",
    "    # Считываем нужные колонки из базы данных городов России\n",
    "    file_id = '1ETmId0_xHc71dYCL78zdZI69Q94VtDgN'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    cities = pd.read_csv(url,  usecols=['federal_district', 'region_type', 'region', 'city', 'capital_marker', 'timezone', 'geo_lat', 'geo_lon', 'population'])\n",
    "\n",
    "    # Меняем на признак, является ли национальным образованием\n",
    "    cities['region_type'] = cities['region_type'].apply(lambda x: 0 if x == 'обл' or x == 'край' or x == 'г'\n",
    "                            else 1)\n",
    "\n",
    "    # Заполняем города, где не указаны\n",
    "    for i in cities[cities.city.isna()].index:\n",
    "        if cities.region[i] == 'Москва':\n",
    "            cities.loc[i, 'city'] = 'Москва'\n",
    "        elif cities.region[i] == 'Санкт-Петербург':\n",
    "            cities.loc[i, 'city'] ='Санкт-Петербург'\n",
    "        elif cities.region[i] == 'Севастополь':\n",
    "            cities.loc[i, 'city'] ='Севастополь'\n",
    "\n",
    "    # Удаляем пустые значения\n",
    "    cities = cities.dropna(subset=['city'])\n",
    "\n",
    "    # Подрезаем названия регионов, оставляем только первое слово\n",
    "    cities.region = cities.region.apply(lambda x: x.split()[0])\n",
    "\n",
    "    # Поскольку в качестве ключа будем использовать город, избавляемся от дубликатов, оставляя более населенные города\n",
    "    cities = cities.sort_values(by='population')\n",
    "    cities = cities.drop_duplicates(subset='city', keep='last')\n",
    "\n",
    "    return cities\n",
    "\n",
    "\n",
    "def timezone_to_regions(df):\n",
    "    # Остаток регионов вытянем из таймзоны\n",
    "    TimeZone_to_Region = df.groupby('CustomerIanaTimeZone')['Region'].agg(mode).reset_index()\n",
    "    df = df.merge(TimeZone_to_Region, how='left', on='CustomerIanaTimeZone')\n",
    "    df.Region_x = df.Region_x.fillna(df.Region_y)\n",
    "    df = df.drop(columns='Region_y')\n",
    "    df = df.rename(columns={'Region_x': 'Region'})\n",
    "\n",
    "    print('timezone_to_regions.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_capital_marker(df):\n",
    "    # Переделаем capital_marker следующим образом:\n",
    "    # поднимем города вверх на 1 ранг, 0 присвоим сельской местности\n",
    "    # присвоим городам федерального значения ранг 4 - Москва, Санкт-Петербург, 3 - Севастополь (на уровне регионального центра)\n",
    "    df.capital_marker = df.capital_marker.apply(lambda x: x+1)\n",
    "\n",
    "    for i in df[(df.City == 'Москва')].index:\n",
    "        df.loc[i, 'capital_marker'] = 4\n",
    "    for i in df[(df.City == 'Санкт-Петербург')].index:\n",
    "        df.loc[i, 'capital_marker'] = 4\n",
    "    for i in df[(df.City == 'Севастополь')].index:\n",
    "        df.loc[i, 'capital_marker'] = 3\n",
    "\n",
    "    df.capital_marker = df.capital_marker.fillna(0).astype('int')\n",
    "\n",
    "    print('process_capital_marker.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "# Функция для исправления региона с Москвы на Мос обл для значений не \"Москва\"\n",
    "def fix_fedcity_obl(row, city, region):\n",
    "    if (row[region] ==  'Москва') & (row[city] != 'Москва'):\n",
    "        return 'Московская'\n",
    "    elif (row[region] ==  'Санкт-Петербург') & (row[city] != 'Санкт-Петербург'):\n",
    "        return 'Ленинградская'\n",
    "    elif (row[region] ==  'Севастополь') & (row[city] != 'Севастополь'):\n",
    "        return 'Крым'\n",
    "    else:\n",
    "        return row[region]\n",
    "\n",
    "def add_geo_data(df):\n",
    "    # После всех дополнений осталось совсем мало незаполненных значений. Заполним их модой\n",
    "    region_na_index = df[df.Region.isna()].index\n",
    "        \n",
    "    region_mode = mode(df['Region'].dropna())\n",
    "    df.Region = df.Region.fillna(region_mode)\n",
    "\n",
    "    # Также в пропущенных регионах заполним модой город\n",
    "    city_mode = mode(df['City'].dropna())\n",
    "    for i in region_na_index:\n",
    "        df.loc[i, 'City'] = city_mode\n",
    "\n",
    "    # Заполняем города-регионы, где не указаны\n",
    "    for i in df.City[df.City.isna()].index:\n",
    "        if df.Region[i] == 'Москва':\n",
    "            df.loc[i, 'City'] = 'Москва'\n",
    "        elif df.Region[i] == 'Санкт-Петербург':\n",
    "            df.loc[i, 'City'] ='Санкт-Петербург'\n",
    "        elif df.Region[i] == 'Севастополь':\n",
    "            df.loc[i, 'City'] ='Севастополь'\n",
    "\n",
    "    # Делаем поправку на Москву\n",
    "    df['Region'] = df.apply(lambda row: fix_fedcity_obl(row, 'City', 'Region'), axis=1)\n",
    "\n",
    "    print('add_geo_data.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_city_popul(df):\n",
    "    # Заполним отсутствующие данные 'City_popul' средним по региону, если это город, и 0, если это не город\n",
    "    city_uniques = df[(df.City_popul.isna()) & (df.City != 'Сельские районы')].City.unique()\n",
    "    region_means = df.groupby(['Region', 'City'])['City_popul'].first().reset_index().groupby('Region')['City_popul'].mean()\n",
    "\n",
    "    for city in city_uniques:\n",
    "        city_index = df[df.City == city].index\n",
    "        for i in city_index:\n",
    "            df.loc[i, 'City_popul'] = int(region_means[df.loc[i, 'Region']])\n",
    "\n",
    "    df.City_popul = df.City_popul.fillna(0)\n",
    "\n",
    "    # Заменяем абсолютные значения на ранги\n",
    "    df['City_popul'] = df['City_popul'].apply(lambda x: x if x == 0\n",
    "                           else 1 if x > 0 and x <= 50_000\n",
    "                           else 2 if x > 50_000 and x <= 100_000\n",
    "                           else 3 if x > 100_000 and x <= 250_000\n",
    "                           else 4 if x > 250_000 and x <= 500_000\n",
    "                           else 5 if x > 500_000 and x <= 750_000\n",
    "                           else 6 if x > 750_000 and x <= 1_000_000\n",
    "                           else 7 if x > 1_000_000 and x <= 3_000_000\n",
    "                           else 8 if x > 3_000_000 and x <= 7_000_000\n",
    "                           else 9)\n",
    "    \n",
    "    print('process_city_popul.shape', df.shape)    \n",
    "    return df\n",
    "\n",
    "# Функция замены числа городов в регионе для федеральных городов\n",
    "def replace_num_cities(df):\n",
    "    mask = df['Region'] == 'Москва'\n",
    "    df.loc[mask, 'Num_of_cities'] = (df.loc[df['Region'] == 'Московская', 'Num_of_cities'].values + 1).mean()\n",
    "    mask = df['Region'] == 'Санкт-Петербург'\n",
    "    df.loc[mask, 'Num_of_cities'] = (df.loc[df['Region'] == 'Ленинградская', 'Num_of_cities'].values + 1).mean()\n",
    "    mask = df['Region'] == 'Севастополь'\n",
    "    df.loc[mask, 'Num_of_cities'] = (df.loc[df['Region'] == 'Крым', 'Num_of_cities'].values + 1).mean()\n",
    "\n",
    "    print('replace_num_cities.shape', df.shape)     \n",
    "    return df\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Конвертируем координаты из градусов в радианы\n",
    "    lat1 = radians(lat1)\n",
    "    lon1 = radians(lon1)\n",
    "    lat2 = radians(lat2)\n",
    "    lon2 = radians(lon2)\n",
    "\n",
    "    # Радиус Земли в километрах\n",
    "    R = 6371.0\n",
    "\n",
    "    # Разница в координатах\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "\n",
    "    # Формула гаверсинусового расстояния\n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "\n",
    "    # Расстояние между точками\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def add_federal_district_data(df, cities):\n",
    "    # Дообогащаем данные по федеральным округам доставки и таймзонам\n",
    "    regions_federal_district = cities.groupby('region').agg({\n",
    "        'federal_district': 'first',  # принадлежность федеральному округу\n",
    "        'region_type': 'first',  # тип региона\n",
    "        'city': 'count',    # кол-во городов в регионе (как показатель урбанизированности)\n",
    "        'population': 'sum'  # число жителей в регионе\n",
    "    }).reset_index()\n",
    "    regions_federal_district = regions_federal_district.rename(columns={'city':'Num_of_cities', 'population':'Region_popul', 'region':'Region'})\n",
    "    \n",
    "    # Исправляем число городов в федеральных городах\n",
    "    regions_federal_district = regions_federal_district.pipe(replace_num_cities)\n",
    "\n",
    "    # Считываем нужные колонки из базы данных ВРП по регионам России за 2023г (https://rosstat.gov.ru/folder/13397)\n",
    "    file_id = '182OjhZqF80AGpKcUlcANo-iZJbLAw1jR'\n",
    "    url = f'https://drive.google.com/uc?id={file_id}'\n",
    "    vrp = pd.read_excel(url)\n",
    "\n",
    "    # Переводим ВРП на душу населения в тыс.руб.\n",
    "    vrp['vrp_per_person'] = round(vrp['vrp_per_person']/1000)\n",
    "\n",
    "    # Дополняем данные по регионам ВРП на душу населения\n",
    "    regions_federal_district = regions_federal_district.merge(vrp, how='inner', on='Region')\n",
    "\n",
    "    df = df.merge(regions_federal_district, how='left', on='Region')\n",
    "    df = df.drop(columns=['OrderCustomFieldsDeliveryAddress'])\n",
    "\n",
    "    # Заменяем абсолютные значения Region_popul на ранги\n",
    "    df['Region_popul'] = df['Region_popul'].apply(lambda x: 1 if x <= 100_000\n",
    "                           else 2 if x > 100_000 and x <= 250_000\n",
    "                           else 3 if x > 250_000 and x <= 500_000\n",
    "                           else 4 if x > 500_000 and x <= 750_000\n",
    "                           else 5 if x > 750_000 and x <= 1_000_000\n",
    "                           else 6 if x > 1_000_000 and x <= 3_000_000\n",
    "                           else 7 if x > 3_000_000 and x <= 7_000_000\n",
    "                           else 8)\n",
    "    \n",
    "    # Заменяем пропуски в координатах средними по региону\n",
    "    df['geo_lat'] = df['geo_lat'].fillna(df.groupby('Region')['geo_lat'].transform('mean'))\n",
    "    df['geo_lon'] = df['geo_lon'].fillna(df.groupby('Region')['geo_lon'].transform('mean'))\n",
    "\n",
    "    # Заданные координаты\n",
    "    lat2, lon2 = df[df['City'] == 'Москва'].loc[0, 'geo_lat'], df[df['City'] == 'Москва'].loc[0, 'geo_lon']\n",
    "\n",
    "    # Создание функции для расчета расстояния от заданной точки до каждой строки DataFrame\n",
    "    def calculate_distance(row):\n",
    "        lat1, lon1 = row['geo_lat'], row['geo_lon']\n",
    "        return haversine_distance(lat1, lon1, lat2, lon2)\n",
    "\n",
    "    # Применение функции к каждой строке и создание нового столбца 'distance'\n",
    "    df['distance'] = df.apply(calculate_distance, axis=1)\n",
    "\n",
    "    print('add_federal_district_data.shape', df.shape)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_delivery_type(df):\n",
    "    df.Delivery = df.Delivery.apply(\n",
    "        lambda x: 1 if x in ['58', '48', '78', '87', '2', '46', 'Привезет курьер']\n",
    "        else 0\n",
    "    )\n",
    "    df['payment_type'] = pd.to_numeric(df['payment_type'], errors='coerce')\n",
    "\n",
    "    # Создаем новый признак \"Покупка в кредит\"\n",
    "    df['Credit'] = df['payment_type'] == 11\n",
    "\n",
    "    # Создаем новый признак \"Оплачен при получении\" (остальные - оплачены в момент заказа на сайте)\n",
    "    df['Paid_on_delivery'] = df['payment_type'] == 1\n",
    "\n",
    "    print('add_delivery_type.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_na_with_modified_value(price_dic, row):\n",
    "    if pd.isna(row['ProductName']):\n",
    "        return price_dic[row['OrderLineBasePricePerItem']]\n",
    "    return row['ProductName']\n",
    "\n",
    "\n",
    "def recover_product_name(df):\n",
    "    # Можно попробовать восстаносить пропуски ProductName, используя совпадения по OrderLineBasePricePerItem\n",
    "    # Заменим nan наиболее частотным товаром по данной цене\n",
    "    na_product_name_prices = df[df.ProductName.isna()].OrderLineBasePricePerItem.unique()\n",
    "    price_dic = {}\n",
    "    for price in na_product_name_prices:\n",
    "        try:\n",
    "            price_dic[price] = df[df.OrderLineBasePricePerItem == price].OrderLineProductName.value_counts().head(1).index[0]\n",
    "        except:\n",
    "            price_dic[price] = 'unknown'\n",
    "\n",
    "    df['ProductName'] = df.apply(lambda row: fill_na_with_modified_value(price_dic, row), axis=1)\n",
    "\n",
    "    print('recover_product_name.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def recover_timezone(df):\n",
    "    # Восстановим таймзону CustomerIanaTimeZone из информации о регионах\n",
    "    Region_to_TimeZone = df.groupby('Region')['CustomerIanaTimeZone'].agg(mode).reset_index()\n",
    "    df = df.merge(Region_to_TimeZone, how='left', on='Region')\n",
    "    df.CustomerIanaTimeZone_x = df.CustomerIanaTimeZone_x.fillna(df.CustomerIanaTimeZone_y)\n",
    "    df = df.drop(columns='CustomerIanaTimeZone_y')\n",
    "    df = df.rename(columns={'CustomerIanaTimeZone_x': 'CustomerIanaTimeZone'})\n",
    "\n",
    "    print('recover_timezone.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_scores(df):\n",
    "    df['CustomerCustomFieldsScorBaby'] = df['CustomerCustomFieldsScorBaby'].fillna(False)\n",
    "    df['CustomerCustomFieldsScorBeauty'] = df['CustomerCustomFieldsScorBeauty'].fillna(False)\n",
    "    df['CustomerCustomFieldsScorStirka'] = df['CustomerCustomFieldsScorStirka'].fillna(False)\n",
    "    df['CustomerCustomFieldsScorUborka'] = df['CustomerCustomFieldsScorUborka'].fillna(False)\n",
    "    # Приводим к нужному типу данных\n",
    "    df['CustomerCustomFieldsScorBaby'] = df['CustomerCustomFieldsScorBaby'].astype('bool')\n",
    "    df['CustomerCustomFieldsScorBeauty'] = df['CustomerCustomFieldsScorBeauty'].astype('bool')\n",
    "    df['CustomerCustomFieldsScorStirka'] = df['CustomerCustomFieldsScorStirka'].astype('bool')\n",
    "    df['CustomerCustomFieldsScorUborka'] = df['CustomerCustomFieldsScorUborka'].astype('bool')\n",
    "\n",
    "    print('fill_scores.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_nans(df):\n",
    "    df['payment_type'] = df['payment_type'].fillna(mode(df['payment_type']))\n",
    "    df['ProductCategoriesIdsWebsite'] = df['ProductCategoriesIdsWebsite'].fillna(0)\n",
    "    df['price_syn_ru'] = df['price_syn_ru'].fillna(df['OrderLineBasePricePerItem'])\n",
    "    df['price_diff'] = df['price_diff'].fillna(0)\n",
    "    df['authorized_days'] = df['authorized_days'].fillna(round(df['authorized_days'].mean()))\n",
    "    df['authorized_website_id_count'] = df['authorized_website_id_count'].fillna(round(df['authorized_website_id_count'].mean()))\n",
    "    df['days_since_last_authorized'] = df['days_since_last_authorized'].fillna(round(df['days_since_last_authorized'].mean()))\n",
    "    df['DeviceCategory'] = df['DeviceCategory'].fillna(mode(df['DeviceCategory'].dropna())).astype('int')\n",
    "    df['LastReferalSource'] = df['LastReferalSource'].fillna('Other')\n",
    "    df['LastSearchEngineRoot'] = df['LastSearchEngineRoot'].fillna('Other')\n",
    "    df['LastSocialNetwork'] = df['LastSocialNetwork'].fillna('Other')\n",
    "    df['LastTrafficSource'] = df['LastTrafficSource'].fillna('Other')\n",
    "    df['PageViews'] = df['PageViews'].fillna(round(df['PageViews'].mean()))\n",
    "    df['VisitDuration'] = df['VisitDuration'].fillna(round(df['VisitDuration'].mean()))\n",
    "    df['VisitIDNums'] = df['VisitIDNums'].fillna(round(df['VisitIDNums'].mean()))\n",
    "    df['WatchIDsNums'] = df['WatchIDsNums'].fillna(round(df['WatchIDsNums'].mean()))\n",
    "    df['LastSourceEngine'] = df['LastSourceEngine'].fillna(-10).astype('int')\n",
    "    df['PurchaseCouponApplied'] = df['PurchaseCouponApplied'].fillna(0)\n",
    "\n",
    "    df = df.loc[~((df['OrderLineStatusIdsExternalId'] == 'Cancelled') | (df['OrderLineStatusIdsExternalId'] == 'Mindbox-Tech-Cancellation'))]\n",
    "\n",
    "    print('fill_nans.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_calendar_features(df):\n",
    "    # Устанавливаем правильный формат\n",
    "    df.OrderFirstActionDateTimeUtc = pd.to_datetime(df.OrderFirstActionDateTimeUtc)\n",
    "\n",
    "    # Создадим дополнительные временные признаки\n",
    "    df['OrderDate'] = pd.to_datetime(df['OrderFirstActionDateTimeUtc'].dt.date)\n",
    "    df['OrderYear'] = df['OrderFirstActionDateTimeUtc'].dt.year\n",
    "    df['OrderMonth'] = df['OrderFirstActionDateTimeUtc'].dt.month\n",
    "    df['OrderDay'] = df['OrderFirstActionDateTimeUtc'].dt.day\n",
    "    df['OrderHour'] = df['OrderFirstActionDateTimeUtc'].dt.hour\n",
    "    df['OrderMinute'] = df['OrderFirstActionDateTimeUtc'].dt.minute\n",
    "    df['OrderDayOfWeek'] = df['OrderFirstActionDateTimeUtc'].dt.weekday\n",
    "    df['OrderWeekNumber'] = df['OrderFirstActionDateTimeUtc'].dt.isocalendar().week\n",
    "\n",
    "    # Рабочий или праздничный/выходной\n",
    "    cal = Russia() # Использование календаря для России\n",
    "    df['IsWorkingDay'] = df['OrderFirstActionDateTimeUtc'].apply(cal.is_working_day)\n",
    "\n",
    "    print('add_calendar_features.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_clear_price(df):\n",
    "    # Имеет смысл ввести новое поле для расчета общей стоимости заказа без вычетов по скидкам и кэшбекам, чтобы убрать это искажение. \n",
    "    # Кроме того, кажется разумным исключить из стоимости заказа цену доставки, поскольку эта цена не является предметом выбора покупателя, \n",
    "    # а зависит исключительно от порога стоимости заказа и региона.\n",
    "\n",
    "    # Сгруппируем данные по заказам\n",
    "    df_total_price = df.groupby('OrderIdsMindboxId', as_index=False).agg({\n",
    "        'OrderTotalPrice': 'first', \n",
    "        'OrderDeliveryCost': 'first', \n",
    "        'OrderLinePriceOfLine': 'sum'})\n",
    "    df_total_price = df_total_price.rename(columns={'OrderLinePriceOfLine' : 'OrderTotalPrice_cleared'})\n",
    "    df_total_price['OrderTotalPrice_cleared'] = df_total_price['OrderTotalPrice_cleared'].round(2)\n",
    "\n",
    "    # Добавим новый столбец 'OrderTotalPrice_cleared' к df\n",
    "    # Дальнейшее исследование будем проводить на этом очищенном признаке\n",
    "\n",
    "    df = df.merge(df_total_price[['OrderIdsMindboxId', 'OrderTotalPrice_cleared']], \n",
    "                                on='OrderIdsMindboxId', how='left')\n",
    "    \n",
    "    # Сгруппируем данные по заказам\n",
    "    df_total_price = df.groupby('OrderIdsMindboxId', as_index=False)['OrderTotalPrice_cleared'].first()\n",
    "\n",
    "    # Вычисляем среднее и стандартное отклонение для 'OrderTotalPrice'\n",
    "    mean_price = df_total_price['OrderTotalPrice_cleared'].mean()\n",
    "    std_deviation = df_total_price['OrderTotalPrice_cleared'].std()\n",
    "\n",
    "    # Задаем пороговое значение Z-Score\n",
    "    threshold = 2.5\n",
    "\n",
    "    # Рассчитываем Z-Score для каждой точки данных\n",
    "    df_total_price['Z_Score'] = (df_total_price['OrderTotalPrice_cleared'] - mean_price) / std_deviation\n",
    "\n",
    "    # Выделяем точки данных, у которых Z-Score выше порога\n",
    "    outliers = df_total_price[df_total_price['Z_Score'] > threshold]\n",
    "\n",
    "    # Пороговое значение OrderTotalPrice_cleared\n",
    "    threshold_total_price = int(outliers['OrderTotalPrice_cleared'].min())\n",
    "\n",
    "    # Хвосту справа имеет смысл присвоить значение, по которому происходит отсечение.\n",
    "    df_total_price.loc[df_total_price['OrderTotalPrice_cleared'] > threshold_total_price, 'OrderTotalPrice_cleared'] = threshold_total_price\n",
    "    df_total_price.rename(columns={'OrderTotalPrice_cleared': 'OrderTotalPrice_cleared_Z'}, inplace=True)\n",
    "\n",
    "    # Добавим новый столбец 'OrderTotalPrice_cleared_Z' к df, где максимальные значения OrderTotalPrice_cleared заменены пороговым\n",
    "\n",
    "    df = df.merge(df_total_price[['OrderIdsMindboxId', 'OrderTotalPrice_cleared_Z']], \n",
    "                                on='OrderIdsMindboxId', how='left')\n",
    "    \n",
    "    print('get_clear_price.shape', df.shape)    \n",
    "    return df\n",
    "\n",
    "\n",
    "def drop_tests(df):\n",
    "    # Присутствуют некорректные значения, видимо, относящиеся к тестированию сервиса. Их нужно удалить из списка заказов.\n",
    "    orders_to_drop = list(df[(df.ProductName == 'unknown') \n",
    "                                    | (df.ProductName == 'тестовый')].OrderIdsMindboxId.index)\n",
    "    df = df.drop(index=orders_to_drop)\n",
    "\n",
    "    print('drop_tests.shape', df.shape) \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_quantity(df):\n",
    "    # Вычисляем среднее и стандартное отклонение для 'OrderLineQuantity'\n",
    "    df['OrderLineQuantity'] = df['OrderLineQuantity'].astype('int')\n",
    "    mean_price = df['OrderLineQuantity'].mean()\n",
    "    std_deviation = df['OrderLineQuantity'].std()\n",
    "\n",
    "    # Задаем пороговое значение Z-Score\n",
    "    threshold = 2\n",
    "\n",
    "    # Рассчитываем Z-Score для каждой точки данных\n",
    "    df['Z_Score'] = (df['OrderLineQuantity'] - mean_price) / std_deviation\n",
    "\n",
    "    # Выделяем точки данных, у которых Z-Score выше порога\n",
    "    outliers = df[df['Z_Score'] > threshold]\n",
    "\n",
    "    df.drop(columns='Z_Score', inplace=True)\n",
    "\n",
    "    # Зададим максимальное пороговое значение\n",
    "    threshold_line_quantity = int(outliers.OrderLineQuantity.min())\n",
    "    print(f\"Threshold value OrderLineQuantity: {threshold_line_quantity}\")\n",
    "\n",
    "    # Хвосту справа имеет присвоим значение, по которому происходит отсечение.\n",
    "    df['OrderLineQuantity_Z'] = df['OrderLineQuantity']\n",
    "    df.loc[df['OrderLineQuantity'] > threshold_line_quantity, 'OrderLineQuantity_Z'] = threshold_line_quantity\n",
    "\n",
    "    print('process_quantity.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_price(df):\n",
    "    # Базовая цена никогда не должна быть меньше продажной цены. Для сравнения добавим новый признак 'PriceOfLinePerItem'\n",
    "    df['PriceOfLinePerItem'] = df['OrderLinePriceOfLine'] / df['OrderLineQuantity']\n",
    "\n",
    "    # Рассчитаем полный размер скидки\n",
    "    df['DiscountAmount'] = round(1 - (df['PriceOfLinePerItem'] / (df['OrderLineBasePricePerItem'] + 0.01)), 4) * 100\n",
    "\n",
    "    # Основные статистики DiscountAmount в разрезе по товарам\n",
    "    product_describe_df = df.groupby('ProductIdsWebsite')['DiscountAmount'].describe().applymap(lambda x: float(f'{x:.2f}')).reset_index()\n",
    "    # Добавляем границы межквартильного размаха\n",
    "    df = df.merge(product_describe_df[['ProductIdsWebsite', '25%', '75%']], how='left')\n",
    "    # Исправляем тип данных\n",
    "    df['25%'] = df['25%'].astype('float')\n",
    "    df['75%'] = df['75%'].astype('float')\n",
    "    # Считаем, выпадает ли размер скидки из межквартильного размаха\n",
    "    df['25%'] = (df['DiscountAmount'] <= df['25%'])* -1  # \n",
    "    df['75%'] = df['DiscountAmount'] >= df['75%']\n",
    "    df['Сustomer_discount_rank'] = df['25%'] + df['75%']\n",
    "    # Считаем среднюю \"склонность к скидкам\" для каждого клиента\n",
    "    customer_discount_rank_df = df.groupby('CustomerIdsMindboxId')['Сustomer_discount_rank'].mean().reset_index()\n",
    "    # Удаляем лишние столбцы из дф\n",
    "    df.drop(columns=['25%', '75%', 'Сustomer_discount_rank'], inplace=True)\n",
    "    # Добавляем расчитанный столбец 'Сustomer_discount_rank' к основному дф\n",
    "    df = df.merge(customer_discount_rank_df, how='left')\n",
    "    \n",
    "    print('process_price.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_gift(df):\n",
    "    # Создаем новый признак, означающий наличие подарка\n",
    "    df['Gift_Inside'] = df['OrderLinePriceOfLine'] == 0\n",
    "    # Общее число, полученных клиентом\n",
    "    df['cust_gift_sum'] = df.groupby('CustomerIdsMindboxId')['Gift_Inside'].transform('sum')\n",
    "    # Соотношение подарков к платным товарам у клиента\n",
    "    df['cust_gift_mean'] = df.groupby('CustomerIdsMindboxId')['Gift_Inside'].transform('mean')\n",
    "    # Как часто товар предлагается клиентам в качестве подарка\n",
    "    df['prod_as_gift_mean'] = df.groupby('ProductIdsWebsite')['Gift_Inside'].transform('mean')\n",
    "    # Число подарков в заказе\n",
    "    df['Gift_Inside'] = df.groupby('OrderIdsMindboxId')['Gift_Inside'].transform('sum')\n",
    "\n",
    "    print('check_gift.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def check_sale(df):\n",
    "    # Новый признак, показывающий, был ли внутри товар с 99% сидкой: 'Sale99_Inside'\n",
    "    # Создаем новый признак - товар продан со скидкой > 98%\n",
    "    df['Sale99_Inside'] = ((df['DiscountAmount'] > 98) & (df['DiscountAmount'] < 100))\n",
    "\n",
    "    # Создаем группировку с кол-вом акционных товаров в каждом заказе\n",
    "    sale99 = df.groupby('OrderIdsMindboxId')['Sale99_Inside'].sum().reset_index()\n",
    "\n",
    "    # Булевый признак переименовываем и меняем на кол-во продаж товара с 99% скидкой \n",
    "    df = df.rename(columns={'Sale99_Inside': 'product_sale99'})\n",
    "    df['product_sale99'] = df.groupby('ProductIdsWebsite')['product_sale99'].transform('sum')\n",
    "\n",
    "    # Добавляем столбец с кол-вом товаров со скидкой 99% в заказе\n",
    "    df = df.merge(sale99, on='OrderIdsMindboxId', how='left')\n",
    "\n",
    "    # Добавляем столбец со средним числом товаров со скидкой 99% в заказах клиента\n",
    "    Customer_Sale99_Inside = df.groupby(['CustomerIdsMindboxId', 'OrderIdsMindboxId'])['Sale99_Inside'].first().groupby('CustomerIdsMindboxId').mean().reset_index()\n",
    "    Customer_Sale99_Inside = Customer_Sale99_Inside.rename(columns={'Sale99_Inside':'cust_sale99_mean'})\n",
    "    df = df.merge(Customer_Sale99_Inside, on='CustomerIdsMindboxId', how='left')\n",
    "\n",
    "    # Добавляем столбец со средним числом товаров со скидкой 99% в заказах клиента\n",
    "    Customer_Sale99_Inside = df.groupby(['CustomerIdsMindboxId', 'OrderIdsMindboxId'])['Sale99_Inside'].first().groupby('CustomerIdsMindboxId').sum().reset_index()\n",
    "    Customer_Sale99_Inside = Customer_Sale99_Inside.rename(columns={'Sale99_Inside':'cust_sale99_sum'})\n",
    "    df = df.merge(Customer_Sale99_Inside, on='CustomerIdsMindboxId', how='left')\n",
    "\n",
    "    print('check_sale.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "def replace_value_cities(pop_value, x):\n",
    "    if x in pop_value:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Города <0.1%'\n",
    "    \n",
    "\n",
    "def replace_value_regions(pop_value, x):\n",
    "    if x in pop_value:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Регионы <0.5%'\n",
    "    \n",
    "def replace_value_zones(pop_value, x):\n",
    "    if x in pop_value:\n",
    "        return x\n",
    "    else:\n",
    "        return 'Other <0.5%'\n",
    "\n",
    "\n",
    "\n",
    "def drop_non_active_goods(df):\n",
    "    # Исключаем данные до 20-08-2022 \n",
    "    df_paid = df[df['OrderFirstActionDateTimeUtc'] > '20-08-2022']\n",
    "\n",
    "    # Исключаем неактивные товары\n",
    "    df_paid = df_paid[df_paid['ProductIsAvailable'] == 1]\n",
    "    df_paid.drop(columns='ProductIsAvailable', inplace=True)\n",
    "\n",
    "    # Исключаем товары с 100% и 99% скидкой\n",
    "    df_paid = df_paid[df_paid['OrderLinePriceOfLine'] != 0]\n",
    "    df_paid.drop(columns='OrderLinePriceOfLine', inplace=True)\n",
    "    df_paid = df_paid[df_paid['DiscountAmount'] < 98]\n",
    "\n",
    "    # Добавляем дополнительный целевой признак - для классификации\n",
    "    df_paid['Сustomer_discount_rank_class'] = df_paid['Сustomer_discount_rank'].apply(lambda x: 0 if x < -0.3\n",
    "                                                                            else 2 if x > 0.3\n",
    "                                                                            else 1)\n",
    "\n",
    "    # Меняем порядок столбцов для удобства\n",
    "    new_column_order = df_paid.columns.to_list()\n",
    "    new_column_order.remove('Сustomer_discount_rank')\n",
    "    new_column_order.remove('Сustomer_discount_rank_class')\n",
    "    new_column_order = ['Сustomer_discount_rank', 'Сustomer_discount_rank_class'] + new_column_order\n",
    "\n",
    "    df_paid = df_paid[new_column_order]\n",
    "\n",
    "    print('df_paid.shape', df_paid.shape)\n",
    "    return df_paid\n",
    "\n",
    "\n",
    "def get_product_rank(df):\n",
    "    # Ранее мы исключили из рассмотрения Cancelled заказы (отмененные). поскольку абсолютное большинство из них были признаны тестовыми.\n",
    "    # Также, вероятно, из данных следует исключить вновь созданные, но еще не обработанные заказы, поскольку часть из них также может быть невалидна.\n",
    "    # (Например, им еще просто не успели назначить статус Cancelled)\n",
    "    # После этого столбец стоит удалить.\n",
    "\n",
    "    df = df[df['OrderLineStatusIdsExternalId'] != 'Create']\n",
    "    df = df.drop(columns='OrderLineStatusIdsExternalId')\n",
    "\n",
    "    # Из OrderLineNumber можно извлечь следующую информацию:\n",
    "    # 1) Общее число уникальных товаров в заказе\n",
    "    # 2) Порядок добавления товара в корзину (что скорее всего стоит интерпретировать как значимость товара для этого заказа / клиента)\n",
    "\n",
    "    # Очевидно, нам важен порядок добавления только для первых товаров, далее значимость этого признака снижается.\n",
    "    # Разобьем данные на перцентили с шагом в 20%, чтобы получить разумные рамки для укрупнения рангов:\n",
    "    percentiles = [i for i in range(0, 101, 20)]\n",
    "\n",
    "    # Вычисляем перцентили\n",
    "    percentile_values = df['OrderLineNumber'].quantile([p/100 for p in percentiles])\n",
    "\n",
    "    # Создаем агрегированный DataFrame\n",
    "    agg_df = pd.DataFrame({'Percentile': percentiles, 'OrderLineNumber': percentile_values})\n",
    "    agg_df['OrderLineNumber'] = agg_df['OrderLineNumber'].apply(lambda x: f'{x:.0f}')\n",
    "\n",
    "    # Также необходимо учесть, что товар мог добавляться не во все заказы, \n",
    "    # т.е. для правильного расчета ранга нужно учесть и те случаи, когда товар в корзину добавлен не был,\n",
    "    # и присвоить ему в этих случаях \"последнее\" место.\n",
    "\n",
    "    # Создадим новый признак 'ProductRank' на основе распределения рангов на перцентили:\n",
    "    df['ProductRank'] = df['OrderLineNumber'].apply(lambda x: 1 if x == 1\n",
    "                                                                else 2 if x >= 2 and x < 4\n",
    "                                                                else 3 if x >= 4 and x < 7\n",
    "                                                                else 4 if x >= 7 and x < 11\n",
    "                                                                else 5 if x >= 11\n",
    "                                                                else np.nan)\n",
    "    \n",
    "    print('get_product_rank.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def process_order_lines(df):\n",
    "    # Посчитаем число строк (продуктов внутри) по заказам\n",
    "    lines_inside = df.groupby('OrderIdsMindboxId')['OrderLineNumber'].max().reset_index()\n",
    "    lines_inside = lines_inside.rename(columns={'OrderLineNumber': 'LineNumbersInside'})\n",
    "\n",
    "    # Добавим новый признак, означающий число товарных позиций в заказе - 'LineNumbersInside'\n",
    "    df = df.merge(lines_inside, on='OrderIdsMindboxId', how='left')\n",
    "    # Вычисляем среднее и стандартное отклонение для 'LineNumbersInside'\n",
    "    mean_number = lines_inside['LineNumbersInside'].mean()\n",
    "    std_deviation = lines_inside['LineNumbersInside'].std()\n",
    "    # Задаем пороговое значение Z-Score\n",
    "    threshold = 2\n",
    "\n",
    "    # Рассчитываем Z-Score для каждой точки данных\n",
    "    lines_inside['Z_Score'] = (lines_inside['LineNumbersInside'] - mean_number) / std_deviation\n",
    "    # Выделяем точки данных, у которых Z-Score выше порога\n",
    "    outliers = lines_inside[lines_inside['Z_Score'] > threshold]\n",
    "    # Присвоим вместо экстремальных значений пороговые\n",
    "    threshold_line_quantity = int(outliers['LineNumbersInside'].min())\n",
    "\n",
    "    # Хвосту справа имеет смысл присвоить значение, по которому происходит отсечение.\n",
    "    lines_inside.loc[lines_inside['LineNumbersInside'] > threshold_line_quantity, 'LineNumbersInside'] = threshold_line_quantity  \n",
    "    # Добавим новый признак, означающий число товаров в заказе - 'LineNumbersInside'\n",
    "    lines_inside = lines_inside.rename(columns={'LineNumbersInside': 'LineNumbersInside_Z'})\n",
    "    lines_inside.drop(columns='Z_Score', inplace=True)\n",
    "    df = df.merge(lines_inside, on='OrderIdsMindboxId', how='left')\n",
    "\n",
    "    print('process_order_lines.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def post_processing(df):\n",
    "    # Часть городов имеют очень низкую частотность, заменим их на единое значение\n",
    "    df_total_price = df.groupby('OrderIdsMindboxId', as_index=False)['City'].first()\n",
    "    # Список городов, составляющих > 0.1%\n",
    "    value_counts = df_total_price['City'].value_counts(normalize=True)\n",
    "    pop_value = value_counts[value_counts > 0.001].index.tolist()\n",
    "    # Применяем функцию к столбцу 'City'\n",
    "    df['City'] = df['City'].apply(lambda x: replace_value_cities(pop_value, x))\n",
    "\n",
    "    # Часть регионов имеют очень низкую частотность, заменим их на единое значение\n",
    "    df_total_price = df.groupby('OrderIdsMindboxId', as_index=False)['Region'].first()\n",
    "    # Список регионов, составляющих > 0.5%\n",
    "    value_counts = df_total_price['Region'].value_counts(normalize=True)\n",
    "    pop_value = value_counts[value_counts > 0.005].index.tolist()\n",
    "    # Применяем функцию к столбцу 'Region'\n",
    "    df['Region'] = df['Region'].apply(lambda x: replace_value_regions(pop_value, x))\n",
    "\n",
    "    # Часть зон имеют очень низкую частотность, заменим их на единое значение\n",
    "    df_total_price = df.groupby('OrderIdsMindboxId', as_index=False)['CustomerIanaTimeZone'].first()\n",
    "    # Список зон, составляющих > 0.5%\n",
    "    value_counts = df_total_price['CustomerIanaTimeZone'].value_counts(normalize=True)\n",
    "    pop_value = value_counts[value_counts > 0.005].index.tolist()\n",
    "    # Применяем функцию к столбцу 'CustomerIanaTimeZone'\n",
    "    df['CustomerIanaTimeZone'] = df['CustomerIanaTimeZone'].apply(lambda x: replace_value_zones(pop_value, x))\n",
    "\n",
    "    # Дни рождения заполнены менее, чем у 4% активных клиентов\n",
    "    df_birthdays = df.groupby('CustomerIdsMindboxId')['CustomerBirthDate'].first()\n",
    "    # Есть значения, превышающие 100 лет или не достигшие совершеннолетия - скорее всего данные недостоверны, удалим эти значения\n",
    "    # Посчитаем возраст клиентов\n",
    "    df['CustomerBirthDate'] = pd.to_datetime(df['CustomerBirthDate'])\n",
    "    current_date = pd.to_datetime('now') # текущая дата\n",
    "    # Возраст приближенно в годах\n",
    "    df['Age'] = round((current_date - df['CustomerBirthDate']) / pd.Timedelta(days=365.25))\n",
    "    # Удалим недостоверные значения\n",
    "    df['Age'] = df['Age'].apply(lambda x: x if x > 16 and x < 100 else np.nan)\n",
    "\n",
    "    df = df.drop(columns=['CustomerCustomFieldsEcoMobile'])\n",
    "\n",
    "    # 'CustomerCustomFieldsGWtickets'\n",
    "    data=df.groupby('CustomerIdsMindboxId')['CustomerCustomFieldsGWtickets'].first().value_counts().reset_index()\n",
    "    # Вычисляем первый (25-й процентиль) и третий (75-й процентиль) квартили\n",
    "    Q1 = data['CustomerCustomFieldsGWtickets'].quantile(0.25)\n",
    "    Q3 = data['CustomerCustomFieldsGWtickets'].quantile(0.75)\n",
    "    # Вычисляем межквартильный диапазон (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    # Задаем пороговые значения для определения выбросов\n",
    "    lower_threshold = Q1 - 2 * IQR\n",
    "    upper_threshold = Q3 + 2 * IQR\n",
    "    # Выделяем точки данных, находящиеся за пределами порогов\n",
    "    outliers = data[(data['CustomerCustomFieldsGWtickets'] < lower_threshold) | (data['CustomerCustomFieldsGWtickets'] > upper_threshold)]\n",
    "    # Данные без выбросов\n",
    "    data=data[(data['CustomerCustomFieldsGWtickets'] > lower_threshold) & (data['CustomerCustomFieldsGWtickets'] < upper_threshold)]\n",
    "    outliers_list = list(outliers['CustomerCustomFieldsGWtickets'].unique())\n",
    "    # Уберем экстремальные значения, заменив средней\n",
    "    GWtickets_mean = int(data['CustomerCustomFieldsGWtickets'].mean())\n",
    "    df['CustomerCustomFieldsGWtickets'] = df['CustomerCustomFieldsGWtickets'].apply(lambda x: GWtickets_mean if x in outliers_list\n",
    "                                                 else x)\n",
    "    \n",
    "    df.drop(columns=['OrderLineNumber',\n",
    "                        'OrderLineBasePricePerItem', # корреляция с price_syn_ru 0.943247\n",
    "                        'OrderTotalPrice',\n",
    "                        'payment_type',\n",
    "                        'ProductName',\n",
    "                        'CustomerFirstName',\n",
    "                        'CustomerMiddleName', \n",
    "                        'CustomerLastName',\n",
    "                        'CustomerBirthDate',\n",
    "                        'PriceOfLinePerItem',\n",
    "                        ],\n",
    "                        inplace=True)\n",
    "\n",
    "    print('post_processing.shape', df.shape)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_pricing_stat(pricing):\n",
    "\n",
    "    pricing_stat = pricing[['id', 'price_syn_ru', 'date']].sort_values(by=['id', 'date'])\n",
    "    # Группируем данные по 'id' и вычисляем разницу между соседними датами\n",
    "    pricing_stat['date_diff'] = pricing_stat.groupby('id')['date'].diff()\n",
    "    # Используем метод shift, чтобы сдвинуть значения на одну позицию вперед\n",
    "    pricing_stat['date_diff'] = pricing_stat.groupby('id')['date_diff'].shift(-1)\n",
    "    # Последние значения (NaT) заполняем нулями\n",
    "    pricing_stat['date_diff'] = pricing_stat['date_diff'].fillna(pd.Timedelta(0))\n",
    "\n",
    "    # Считаем время экспозиции каждой цены в днях\n",
    "    pricing_stat = pricing_stat.groupby(['id', 'price_syn_ru'], as_index=False).agg(\n",
    "        price_exp_pd = ('date_diff', 'sum')\n",
    "        )\n",
    "    pricing_stat['price_exp_pd'] = pricing_stat['price_exp_pd'].dt.total_seconds() / 60 / 60 / 24\n",
    "\n",
    "    return pricing_stat\n",
    "\n",
    "def add_product_stats(products_visits, df, pricing_stat):\n",
    "\n",
    "    visits_count = products_visits.groupby(['ProductIdsWebsite', 'price_syn_ru'], as_index=False).agg(visits_count= ('date_time', 'count'))\n",
    "    print('visits_count.shape', visits_count.shape)\n",
    "    # Считаем число заказов по каждой цене\n",
    "    orders_count = df[['OrderIdsMindboxId', 'OrderFirstActionDateTimeUtc', 'ProductIdsWebsite', 'price_syn_ru']][df['OrderFirstActionDateTimeUtc'] >= products_visits['date_time'].min()]\n",
    "    orders_count = orders_count.groupby(['ProductIdsWebsite', 'price_syn_ru'], as_index=False).agg(orders_count= ('OrderIdsMindboxId', 'count'))\n",
    "    print('orders_count.shape', orders_count.shape)\n",
    "\n",
    "    pricing_stat = pricing_stat.merge(visits_count, left_on=['id', 'price_syn_ru'], right_on=['ProductIdsWebsite', 'price_syn_ru'], how='left')\n",
    "    print('pricing_stat.merge(visits_count.shape', pricing_stat.shape)\n",
    "    pricing_stat = pricing_stat.merge(orders_count, left_on=['id', 'price_syn_ru'], right_on=['ProductIdsWebsite', 'price_syn_ru'], how='left')\n",
    "    print('pricing_stat.merge(orders_count.shape', pricing_stat.shape)\n",
    "    pricing_stat.drop(columns=['ProductIdsWebsite_x', 'ProductIdsWebsite_y'], inplace=True)\n",
    "    pricing_stat_notna = pricing_stat.fillna(0)\n",
    "\n",
    "    # Посчитаем корреляции числа заказов товаров и цены, числа визитов, времени экспозиции на сайте.\n",
    "    # Если корреляция не может быть корректно посчитана, считаем, что ее нет (равна 0)\n",
    "    price_corr_id = pricing_stat_notna.groupby('id').apply(lambda x: x['orders_count'].corr(x['price_syn_ru'])).fillna(0)\n",
    "    visits_corr_id = pricing_stat_notna.groupby('id').apply(lambda x: x['orders_count'].corr(x['visits_count'])).fillna(0)\n",
    "    exp_pd_corr_id = pricing_stat_notna.groupby('id').apply(lambda x: x['orders_count'].corr(x['price_exp_pd'])).fillna(0)\n",
    "\n",
    "    # Средние продажи в день (берем в знаменатель дни экспозиции товара)\n",
    "    orders_count_id = pricing_stat_notna.groupby('id')['orders_count'].sum() / pricing_stat_notna.groupby('id')['price_exp_pd'].sum()\n",
    "\n",
    "    # Вычисляем взвешенное среднее цены с учетом периодов действия внутри каждой группы 'id'\n",
    "    avg_price_id = pricing_stat_notna.groupby('id').apply(lambda x: (x['price_syn_ru'] * x['price_exp_pd']).sum() / x['price_exp_pd'].sum())\n",
    "\n",
    "    # Вычисляем стандартное отклонение цены 'price_syn_ru' по группам 'id'\n",
    "    std_price_id = pricing_stat_notna.groupby('id')['price_syn_ru'].std()\n",
    "\n",
    "    # Готовим к присоединению к единой таблице\n",
    "    order_corr_id = pd.concat([avg_price_id, std_price_id, price_corr_id, visits_corr_id, exp_pd_corr_id, orders_count_id], axis=1).reset_index()\n",
    "    order_corr_id.columns = ['ProductIdsWebsite','avg_price', 'std_price', 'price_corr', 'visits_corr', 'exp_pd_corr', 'orders_per_day']\n",
    "    # Присваиваем в неопределенные корреляции средние по столбцам\n",
    "    order_corr_id = order_corr_id.fillna(order_corr_id.mean())\n",
    "    print('order_corr_id.shape', order_corr_id.shape)\n",
    "    print('df.shape', df.shape)\n",
    "\n",
    "    # Присоединяем новые признаки\n",
    "    df = df.merge(order_corr_id, on='ProductIdsWebsite', how='left')\n",
    "    print('df.merge(order_corr_id.shape', df.shape)\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_nans_and_add_features(df):\n",
    "\n",
    "    # Общее фактическое число заказов клиента\n",
    "    df['total_purchase_nums'] = df.groupby(['CustomerIdsMindboxId'])['OrderIdsMindboxId'].transform('nunique')\n",
    "\n",
    "    # Первая/последняя покупка клиента\n",
    "    df['cust_first_order'] = df.groupby('CustomerIdsMindboxId')['OrderFirstActionDateTimeUtc'].transform('first')\n",
    "    df['cust_last_order'] = df.groupby('CustomerIdsMindboxId')['OrderFirstActionDateTimeUtc'].transform('last')\n",
    "\n",
    "    # Среднее время между покупками клиента в днях\n",
    "    df['cust_days_betw_orders'] = (df['cust_last_order'] - df['cust_first_order']).dt.total_seconds() / df['total_purchase_nums'] / 60 / 60 / 24\n",
    "\n",
    "    # Число дней с первой/последней покупки\n",
    "    df['cust_first_order'] = (pd.to_datetime('today') - df['cust_first_order']).dt.total_seconds() / 60 / 60 / 24\n",
    "    df['cust_last_order'] = (pd.to_datetime('today') - df['cust_last_order']).dt.total_seconds() / 60 / 60 / 24\n",
    "\n",
    "    # Средняя стоимость заказа клиента (средний чек)\n",
    "    Customer_OrderTotalPrice_cleared = df.groupby(['CustomerIdsMindboxId', 'OrderIdsMindboxId'])['OrderTotalPrice_cleared'].first().groupby('CustomerIdsMindboxId').mean().reset_index()\n",
    "    Customer_OrderTotalPrice_cleared = Customer_OrderTotalPrice_cleared.rename(columns={'OrderTotalPrice_cleared':'cust_total_price_mean'})\n",
    "    df = df.merge(Customer_OrderTotalPrice_cleared, on='CustomerIdsMindboxId', how='left')\n",
    "\n",
    "    # Общая стоимость заказов клиента\n",
    "    Customer_OrderTotalPrice_cleared = df.groupby(['CustomerIdsMindboxId', 'OrderIdsMindboxId'])['OrderTotalPrice_cleared'].first().groupby('CustomerIdsMindboxId').sum().reset_index()\n",
    "    Customer_OrderTotalPrice_cleared = Customer_OrderTotalPrice_cleared.rename(columns={'OrderTotalPrice_cleared':'cust_total_price_sum'})\n",
    "    df = df.merge(Customer_OrderTotalPrice_cleared, on='CustomerIdsMindboxId', how='left')\n",
    "\n",
    "    # Кол-во предыдущих заказов клиента\n",
    "    CustomerIdsMindboxId_grouped = df.groupby(['CustomerIdsMindboxId', 'OrderIdsMindboxId'], as_index=False)['OrderFirstActionDateTimeUtc'].first()\n",
    "    CustomerIdsMindboxId_cumcount = CustomerIdsMindboxId_grouped.groupby('CustomerIdsMindboxId').cumcount()\n",
    "    CustomerIdsMindboxId_grouped = pd.concat([CustomerIdsMindboxId_grouped, CustomerIdsMindboxId_cumcount], axis=1)\n",
    "    CustomerIdsMindboxId_grouped.rename(columns={0:'cust_orders_cumcount'}, inplace=True)\n",
    "    df = df.merge(CustomerIdsMindboxId_grouped, on=['CustomerIdsMindboxId', 'OrderIdsMindboxId', 'OrderFirstActionDateTimeUtc'], how='left')\n",
    "\n",
    "    # Общее число заказов товара\n",
    "    df['product_total_order_num'] = df.groupby('ProductIdsWebsite')['OrderIdsMindboxId'].transform('count')\n",
    "\n",
    "    # Общее кол-во проданного товара\n",
    "    df['product_total_quantity'] = df.groupby('ProductIdsWebsite')['OrderLineQuantity'].transform('sum')\n",
    "\n",
    "    # Средняя цена продажи товара\n",
    "    df['product_avg_order_price'] = round(df.groupby('ProductIdsWebsite')['price_syn_ru'].transform('mean'), 2)\n",
    "\n",
    "    # Среднее отклонение продажной цены товара\n",
    "    df['product_std_order_price'] = round(df.groupby('ProductIdsWebsite')['price_syn_ru'].transform('std'), 2)\n",
    "\n",
    "    # Время, в течение которого товар продается (в днях)\n",
    "    df['product_selling_exp'] = (df.groupby('ProductIdsWebsite')['OrderFirstActionDateTimeUtc'].transform('last') \\\n",
    "                                        - df.groupby('ProductIdsWebsite')['OrderFirstActionDateTimeUtc'].transform('first')).dt.days\n",
    "\n",
    "    # Заполняем NAN\n",
    "    df['IsNewUser'] = df['IsNewUser'].astype('bool').fillna(df['cust_orders_cumcount'] < 1)\n",
    "    df['PurchaseNums'] = df['PurchaseNums'].fillna(df['IsNewUser'] == 0).astype('int')\n",
    "    df['avg_price'] = df['avg_price'].fillna(df['product_avg_order_price'])\n",
    "    df['std_price'] = df['std_price'].fillna(df['product_std_order_price'])\n",
    "    df['price_corr'] = df['price_corr'].fillna(0)\n",
    "    df['visits_corr'] = df['visits_corr'].fillna(0)\n",
    "    df['exp_pd_corr'] = df['exp_pd_corr'].fillna(0)\n",
    "    df['orders_per_day'] = df['orders_per_day'].fillna(df['product_total_order_num'] / df['product_selling_exp'])\n",
    "    df['Age'] = df['Age'].fillna(-1)\n",
    "\n",
    "    print('fill_nans_and_add_features', df.shape)\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print('fill_nans_and_add_features', df.shape)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_lines.shape (1012924, 19)\n",
      "orders.shape (53457, 14)\n",
      "products.shape (602, 4)\n",
      "clients.shape (230150, 24)\n",
      "pricing.shape (821595, 9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "visits.shape (2589765, 16)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yandex_client_ids.shape (160479, 4)\n",
      "yandex_client_ids.shape (159535, 4)\n",
      "df.shape (513591, 14)\n",
      "products.shape (602, 5)\n",
      "df.shape (509920, 18)\n",
      "df.shape (457359, 41)\n",
      "get_nearest_pricing_time.shape (457359, 42)\n",
      "df.shape (457359, 44)\n",
      "add_calendar_features.shape (457359, 53)\n",
      "yandex_client_ids.shape (70814, 7)\n",
      "yandex_client_ids_to_merge.shape (46544, 4)\n",
      "df.shape (457359, 56)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(yandex_id_dic) 41558\n",
      "visits.shape (220599, 17)\n",
      "visits.shape (141468, 16)\n",
      "df.shape (457359, 70)\n"
     ]
    }
   ],
   "source": [
    "# load dfs\n",
    "order_lines = get_order_lines()\n",
    "orders = get_orders()\n",
    "products = get_products()\n",
    "clients = get_clients()\n",
    "pricing = get_pricing()\n",
    "products_visits = get_products_visits()\n",
    "visits = get_visits()\n",
    "yandex_client_ids = get_yandex_client_ids()\n",
    "\n",
    "# Соединяем order_lines и orders\n",
    "# Убирание updated_at в order_lines привело к возникновению дубликатов, вычистим их\n",
    "df = order_lines[['ids.mindboxId', \n",
    "                'customer.ids.mindboxId',\n",
    "                'firstAction.dateTimeUtc',\n",
    "                'deliveryCost',\n",
    "                'lines.product.ids.website',\n",
    "                'lines.quantity',\n",
    "                'lines.basePricePerItem',\n",
    "                'lines.priceOfLine',\n",
    "                'lines.status.ids.externalId',\n",
    "                'lines.number']].drop_duplicates().merge(\n",
    "                    orders[['ids.mindboxId', \n",
    "                            'totalPrice',\n",
    "                            'customFields.deliveryAddress',\n",
    "                            'customFields.deliveryType',\n",
    "                            'payment_type']], how='inner')\n",
    "\n",
    "print('df.shape', df.shape)\n",
    "\n",
    "products = preprocess_products_for_merge(products)\n",
    "\n",
    "df = df.merge(products, \n",
    "                    how='inner', \n",
    "                    left_on='lines.product.ids.website', \n",
    "                    right_on='ProductIdsWebsite')\n",
    "df.drop(columns='ProductIdsWebsite', inplace=True)\n",
    "\n",
    "print('df.shape', df.shape)\n",
    "\n",
    "# Добавим данные по clients, исключив клиентов, которые не делали покупок, и тех, кого не получилось идентифицировать по id\n",
    "df = df.merge(clients[['CustomerIdsMindboxId',\n",
    "            'CustomerSex',\n",
    "            'CustomerFirstName',\n",
    "            'CustomerMiddleName',\n",
    "            'CustomerLastName',\n",
    "            'CustomerIanaTimeZone',\n",
    "            'CustomerBirthDate',\n",
    "            'CustomerIsEmailInvalid',\n",
    "            'CustomerIsMobilePhoneInvalid',\n",
    "            'CustomerBalanceLoyaltyProgramAccountTotal',\n",
    "            'CustomerCustomFieldsCity',\n",
    "            'CustomerCustomFieldsCityOrder',\n",
    "            'CustomerCustomFieldsCitySystem',\n",
    "            'CustomerCustomFieldsEcoMobile',\n",
    "            'CustomerCustomFieldsGWtickets',\n",
    "            'CustomerCustomFieldsLoyaltyCommunication',\n",
    "            'CustomerCustomFieldsLoyaltyProduct',\n",
    "            'CustomerCustomFieldsScorBaby',\n",
    "            'CustomerCustomFieldsScorBeauty',\n",
    "            'CustomerCustomFieldsScorStirka',\n",
    "            'CustomerCustomFieldsScorUborka',\n",
    "            'CustomerCustomFieldsUtmFirst',\n",
    "            'CustomerCustomFieldsUtmMedium',\n",
    "            'CustomerCustomFieldsUtmSource']], \n",
    "            left_on='customer.ids.mindboxId', \n",
    "            right_on='CustomerIdsMindboxId', \n",
    "            how='inner')\n",
    "df = df.drop(columns='CustomerIdsMindboxId')\n",
    "\n",
    "print('df.shape', df.shape)\n",
    "\n",
    "# Меняем тип данных для дат\n",
    "df['firstAction.dateTimeUtc'] = pd.to_datetime(df['firstAction.dateTimeUtc'])\n",
    "# Приводим к единому формату id\n",
    "df['lines.product.ids.website'] = df['lines.product.ids.website'].astype(float).astype(int)\n",
    "# process pricing time\n",
    "df = get_nearest_pricing_time(df, pricing, date_time_column_name='firstAction.dateTimeUtc')\n",
    "# Добавляем данные по актуальным ценам во время заказа\n",
    "df = df.merge(pricing[['id', 'date', 'price_syn_ru', 'price_diff']].drop_duplicates(),\n",
    "        left_on=['lines.product.ids.website', 'nearest_pricing_date'],\n",
    "        right_on=['id', 'date'],\n",
    "        how='left')\n",
    "df.drop(columns=['id', 'date'], inplace=True)\n",
    "\n",
    "print('df.shape', df.shape)\n",
    "\n",
    "# Переименуем колонки для удобства\n",
    "df = df.rename(columns={'ids.mindboxId':'OrderIdsMindboxId',\n",
    "                'customer.ids.mindboxId':'CustomerIdsMindboxId',\n",
    "                'firstAction.dateTimeUtc':'OrderFirstActionDateTimeUtc', \n",
    "                'deliveryCost':'OrderDeliveryCost', \n",
    "                'lines.product.ids.website':'ProductIdsWebsite',\n",
    "                'lines.quantity':'OrderLineQuantity', \n",
    "                'lines.basePricePerItem':'OrderLineBasePricePerItem', \n",
    "                'lines.priceOfLine':'OrderLinePriceOfLine',\n",
    "                'lines.status.ids.externalId':'OrderLineStatusIdsExternalId', \n",
    "                'lines.number':'OrderLineNumber',\n",
    "                'totalPrice':'OrderTotalPrice',\n",
    "                'customFields.deliveryAddress':'OrderCustomFieldsDeliveryAddress',\n",
    "                'customFields.deliveryType':'Delivery'})\n",
    "\n",
    "# keep active orders only\n",
    "df = add_calendar_features(df)\n",
    "\n",
    "# Отсортируем данные по дате\n",
    "df = df.sort_values(by=['OrderFirstActionDateTimeUtc', 'OrderIdsMindboxId', 'OrderLineNumber'])\n",
    "\n",
    "# Обрабатываем yandex_client_ids\n",
    "yandex_client_ids = preprocess_yandex_client_ids(yandex_client_ids)\n",
    "\n",
    "# Добавляем данные из yandex_client_ids к df\n",
    "yandex_client_ids_to_merge = prepare_yandex_client_ids_to_merge(yandex_client_ids)\n",
    "df = df.merge(yandex_client_ids_to_merge, on='CustomerIdsMindboxId', how='left')\n",
    "\n",
    "print('df.shape', df.shape)\n",
    "\n",
    "# Создаем словарь замены yandex_id на mindbox_id\n",
    "yandex_id_dic = make_yandex_client_ids_dic(yandex_client_ids)\n",
    "\n",
    "# Добавляем данные из visits к df\n",
    "visits = preprocess_visits(visits, yandex_id_dic)\n",
    "df = df.merge(visits, on=['CustomerIdsMindboxId', 'OrderDate'], how='left')\n",
    "\n",
    "print('df.shape', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_sex.shape (457359, 73)\n",
      "delete_test.shape (453123, 73)\n",
      "get_cities.shape (453123, 68)\n",
      "get_bools.shape (453123, 71)\n",
      "process_regions.shape (453123, 72)\n",
      "timezone_to_regions.shape (453123, 71)\n",
      "add_geo_data.shape (453123, 71)\n",
      "process_capital_marker.shape (453123, 75)\n",
      "process_city_popul.shape (453123, 75)\n",
      "replace_num_cities.shape (85, 5)\n",
      "add_federal_district_data.shape (453123, 80)\n",
      "add_delivery_type.shape (453123, 82)\n",
      "recover_product_name.shape (453123, 82)\n",
      "recover_timezone.shape (453123, 82)\n",
      "fill_scores.shape (453123, 82)\n",
      "fill_nans.shape (423199, 82)\n",
      "get_clear_price.shape (423199, 84)\n",
      "drop_tests.shape (423199, 84)\n",
      "process_price.shape (423199, 87)\n"
     ]
    }
   ],
   "source": [
    "# extract customers' sex\n",
    "df = get_sex(df)\n",
    "\n",
    "# delete test rows\n",
    "df = delete_test(df)\n",
    "\n",
    "# Удалим ненужные столбцы\n",
    "df = df.drop(columns=['CustomerSex', 'CustomerSex2', 'CustomerFirstName_mod'])\n",
    "\n",
    "# convert birthdate to datetime\n",
    "df.CustomerBirthDate = pd.to_datetime(df.CustomerBirthDate)\n",
    "\n",
    "# Заполняем пустоты TODO separate function\n",
    "df['CustomerIsEmailInvalid'] = df['CustomerIsEmailInvalid'].fillna(1) # пустые - невалидны\n",
    "df['CustomerIsEmailInvalid'] = df['CustomerIsEmailInvalid'].astype('bool')\n",
    "df['CustomerIsMobilePhoneInvalid'] = df['CustomerIsMobilePhoneInvalid'].fillna(1) # пустые - невалидны\n",
    "df['CustomerIsMobilePhoneInvalid'] = df['CustomerIsMobilePhoneInvalid'].astype('bool')\n",
    "df.CustomerBalanceLoyaltyProgramAccountTotal = df.CustomerBalanceLoyaltyProgramAccountTotal.fillna(0) # отстуствие баланса меняем на 0\n",
    "\n",
    "# extract cities\n",
    "df = get_cities(df)\n",
    "\n",
    "# Заполним отсутсвующие значения 0 TODO separate function\n",
    "df['CustomerCustomFieldsEcoMobile'] = df['CustomerCustomFieldsEcoMobile'].fillna(0)\n",
    "df['CustomerCustomFieldsGWtickets'] = df['CustomerCustomFieldsGWtickets'].fillna(0)\n",
    "df['CustomerCustomFieldsLoyaltyCommunication'] = df['CustomerCustomFieldsLoyaltyCommunication'].fillna(0)\n",
    "df['CustomerCustomFieldsLoyaltyProduct'] = df['CustomerCustomFieldsLoyaltyProduct'].fillna(0)\n",
    "# Приводим к нужному типу\n",
    "df['CustomerCustomFieldsLoyaltyCommunication'] = df['CustomerCustomFieldsLoyaltyCommunication'].astype('int')\n",
    "df['CustomerCustomFieldsLoyaltyProduct'] = df['CustomerCustomFieldsLoyaltyProduct'].astype('int')\n",
    "\n",
    "# get boolean valeus\n",
    "df = get_bools(df)\n",
    "\n",
    "# Если не было доставки - стоимость 0\n",
    "df.OrderDeliveryCost = df.OrderDeliveryCost.fillna(0) \n",
    "\n",
    "# process regions\n",
    "df = process_regions(df)\n",
    "\n",
    "# merge with cities db\n",
    "cities = get_cities_db()\n",
    "\n",
    "df = df.merge(cities[['region', 'city']], how='left', left_on='City', right_on='city')\n",
    "df.Region = df.Region.fillna(df.region)\n",
    "df = df.drop(columns=['region', 'city', 'CustomerCustomFieldsCity'])\n",
    "\n",
    "# get regions from timezone\n",
    "df = timezone_to_regions(df)\n",
    "\n",
    "# add geo data\n",
    "df = add_geo_data(df)\n",
    "\n",
    "# Дообогащаем данные для городов\n",
    "df = df.merge(cities[['city', 'capital_marker', 'population', 'geo_lat', 'geo_lon']], how='left', left_on='City', right_on='city')\n",
    "df = df.drop(columns=['city'])\n",
    "df = df.rename(columns={'population':'City_popul'})\n",
    "\n",
    "# process capital markers\n",
    "df = process_capital_marker(df)\n",
    "\n",
    "# process city population\n",
    "df = process_city_popul(df)\n",
    "\n",
    "# add geo_lat, geo_lon, vrp, region_popul and other data based on federal districts\n",
    "df = add_federal_district_data(df, cities)\n",
    "\n",
    "# add delviery type data\n",
    "df = add_delivery_type(df)\n",
    "\n",
    "# recover product name\n",
    "df = recover_product_name(df)\n",
    "\n",
    "# recover timezone\n",
    "df = recover_timezone(df)\n",
    "\n",
    "# fill empty scores\n",
    "df = fill_scores(df)\n",
    "\n",
    "# fill NANs\n",
    "df = fill_nans(df)\n",
    "\n",
    "# get price cleared\n",
    "df = get_clear_price(df)\n",
    "\n",
    "# drop test rows\n",
    "df = drop_tests(df)\n",
    "\n",
    "df = process_quantity(df)\n",
    "\n",
    "# process the price data\n",
    "df = process_price(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check_gift.shape (423199, 91)\n",
      "check_sale.shape (423199, 95)\n",
      "get_product_rank.shape (423199, 95)\n",
      "process_order_lines.shape (423199, 97)\n",
      "post_processing.shape (423199, 87)\n",
      "get_nearest_pricing_time.shape (1420926, 3)\n",
      "products_visits.shape (1420926, 3)\n",
      "products_visits.shape (1420926, 4)\n",
      "pricing_stat.shape (17798, 3)\n",
      "visits_count.shape (12172, 3)\n",
      "orders_count.shape (11489, 3)\n",
      "pricing_stat.merge(visits_count.shape (17798, 5)\n",
      "pricing_stat.merge(orders_count.shape (17798, 7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "order_corr_id.shape (561, 7)\n",
      "df.shape (423199, 87)\n",
      "df.merge(order_corr_id.shape (423199, 93)\n",
      "fill_nans_and_add_features (423199, 105)\n",
      "fill_nans_and_add_features (421814, 105)\n",
      "df_paid.shape (372238, 104)\n"
     ]
    }
   ],
   "source": [
    "# add gift info\n",
    "df = check_gift(df)\n",
    "\n",
    "# add sale99 info\n",
    "df = check_sale(df)\n",
    "\n",
    "# get product rank\n",
    "df = get_product_rank(df)\n",
    "\n",
    "# get lines number \n",
    "df = process_order_lines(df)\n",
    "\n",
    "# miscellaneous processing\n",
    "df = post_processing(df)\n",
    "\n",
    "# add product features\n",
    "# Считаем число визитов по каждой цене\n",
    "products_visits = get_nearest_pricing_time(products_visits, pricing, date_time_column_name='date_time')\n",
    "print('products_visits.shape', products_visits.shape)\n",
    "\n",
    "\n",
    "products_visits = products_visits.merge(pricing[['id', 'date', 'price_syn_ru']].drop_duplicates(),\n",
    "         left_on=['ProductIdsWebsite', 'nearest_pricing_date'],\n",
    "         right_on=['id', 'date'],\n",
    "         how='left')\n",
    "products_visits.drop(columns=['id', 'date'], inplace=True)\n",
    "print('products_visits.shape', products_visits.shape)\n",
    "\n",
    "# Считаем время экспозиции цен\n",
    "pricing_stat = get_pricing_stat(pricing)\n",
    "print('pricing_stat.shape', pricing_stat.shape)\n",
    "\n",
    "df = add_product_stats(products_visits, df, pricing_stat)\n",
    "\n",
    "\n",
    "df = fill_nans_and_add_features(df)\n",
    "\n",
    "# drop inactive goods\n",
    "df = drop_non_active_goods(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
